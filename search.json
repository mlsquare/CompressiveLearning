[
  {
    "objectID": "bolt/notebooks/dev.html",
    "href": "bolt/notebooks/dev.html",
    "title": "Boolean Variation",
    "section": "",
    "text": "Boolean Variation\nBOLD: Boolean Logic Deep Learning introduced a mathematical theory to caculuate graidents on Boolean Variables. Below are some necessary info to realize the truth tables, define a Boolean Deep Learning model, and train the network using Backprop. See the paper for details. The following are taken from the paper.",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bolt/notebooks/dev.html#definition",
    "href": "bolt/notebooks/dev.html#definition",
    "title": "Boolean Variation",
    "section": "Definition",
    "text": "Definition\n\nThree-value and Mixed logic\n\nDefine \\(\\mathcal{M} \\equiv \\mathcal{B} \\cup \\{0\\}\\) with logic connectives defined according to those of Boolean logic as follows.\n\nFirst, the negation is: \\(\\neg True = False\\), \\(\\neg False = True\\), and \\(\\neg 0 = 0\\).\nSecond, let \\(\\mathcal{L}\\) be a logic connective, denote by \\(\\mathcal{L}_{\\mathcal{M}}\\) and \\(\\mathcal{L}_{\\mathcal{B}}\\) when it is in \\(\\mathcal{M}\\) and in \\(\\mathcal{B}\\), respectively, then \\(\\mathcal{L}_{\\mathcal{Mb}}(a,b) = \\mathcal{L}_{\\mathcal{Bb}}(a,b)\\) for \\(a, b \\in \\mathcal{B}\\) and \\(\\mathcal{L}_{\\mathcal{M}}(a,b) = 0\\) otherwise.\n\nDenote by \\(\\mathcal{L}\\) a logic set (e.g., \\(\\mathcal{B}\\) or \\(\\mathcal{M}\\)), \\(\\mathcal{R}\\) the real set, \\(\\mathcal{Z}\\) the set of integers, \\(\\mathcal{N}\\) a numeric set (e.g., \\(\\mathcal{R}\\) or \\(\\mathcal{Z}\\)), and \\(\\mathcal{D}\\) a certain set of \\(\\mathcal{L}\\) or \\(\\mathcal{N}\\).\nFor \\(x \\in \\mathcal{N}\\), its logic value denoted by \\(x_{logic}\\) is given as \\(x_{logic} = True \\Leftrightarrow x &gt; 0\\), \\(x_{logic} = False \\Leftrightarrow x &lt; 0\\), and \\(x_{logic} = 0 \\Leftrightarrow x = 0\\).\nThe magnitude of a variable \\(x\\), denoted \\(|x|\\), is defined as its usual absolute value if \\(x \\in \\mathcal{N}\\). And for \\(x \\in \\mathcal{L}\\): \\(|x| = 0\\) if \\(x = 0\\), and \\(|x| = 1\\) otherwise.\nFor \\(\\mathcal{L}\\) a logic connective of \\(\\mathcal{L}\\) and variables \\(a\\), \\(b\\), operation \\(c = \\mathcal{L}(a, b)\\) is defined such that \\(|c| = |a||b|\\) and \\(c_{logic} = \\mathcal{L}(a_{logic}, b_{logic})\\).\n\n\n\nCalculus\n\nA variable x is (extended) Boolean variable with the following encoding \\(x \\in \\{-1,0,1\\}\\) where \\(-1\\) represents logical \\(False\\), \\(+1\\) represents logical \\(True\\), and \\(0\\) represents \\(Ignore\\). We can call this extended Boolean domain.\nOrder relations \\(&lt;\\) and \\(&gt;\\) in \\(\\mathcal{B}\\) are defined as follows: \\(False &lt; True\\), and \\(True &gt; False\\).\nFor \\(a, b \\in \\mathcal{B}\\), the variation from \\(a\\) to \\(b\\), denoted \\(\\delta(a \\to b)\\), is defined as: \\(\\delta(a \\to b) \\equiv True\\) if \\(b &gt; a\\), \\(\\equiv 0\\) if \\(b = a\\), and \\(\\equiv False\\) if \\(b &lt; a\\).\nFor \\(f \\in \\mathcal{F}(\\mathcal{B}, \\mathcal{D})\\), \\(\\forall x \\in \\mathcal{B}\\), write \\(\\delta f(x \\to \\neg x) := \\delta(f(x) \\to f(\\neg x))\\). The variation of \\(f\\) w.r.t \\(x\\), denoted \\(f'(x)\\), is defined as: \\(f'(x) \\equiv \\text{xnor}(\\delta(x \\to \\neg x), \\delta f(x \\to \\neg x))\\).\n\nFor simplicity, we will write \\(\\delta f\\) to denote \\(\\delta f(x \\to \\neg x)\\). Similarly, \\(\\delta x\\) to denote \\(\\delta (x \\to \\neg x)\\)\nFor details see, Section 3.2 of BOLD: Boolean Logic Deep Learning\n\n\nAND Gate\n\nTruth Table\nl \\(AND(x_1,x_2) \\equiv 0\\) if any of the inputs are 0, by definition.\n\n\nDerivative\nRecall: 1. \\(\\delta(a \\to b) \\equiv True\\) if \\(b &gt; a\\), \\(\\equiv 0\\) if \\(b = a\\), and \\(\\equiv False\\) if \\(b &lt; a\\). 2. \\(f'(x) \\equiv \\text{xnor}(\\delta(x \\to \\neg x), \\delta f(x \\to \\neg x))\\).\nThe Truth Table for \\(f(x) = f_a(x) = AND(x,a)\\) is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(a\\)\n\\(x\\)\n\\(\\neg x\\)\n\\(\\delta x\\)\n\\(f(x)\\)\n\\(f(\\neg x)\\)\n\\(\\delta f\\)\n\\(f'\\)\n\n\n\n\nT\nT\nF\nF\nT\nF\nF\nT\n\n\nT\nF\nT\nT\nF\nT\nT\nT\n\n\nF\nT\nF\nF\nF\nF\n0\n0\n\n\nF\nF\nT\nT\nF\nF\n0\n0\n\n\n\nTherefore, \\(f'_{a}(x) = \\text{T} \\text{ iff } a=T, 0 \\text{ o.w}\\)\n\n\n\nOR Gate\n\nTruth Table\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y_{OR}\\)\n\n\n\n\nT\nT\nT\n\n\nT\nF\nT\n\n\nF\nT\nT\n\n\nF\nF\nF\n\n\n\n\\(AND(x_1,x_2) \\equiv 0\\) if any of the inputs are 0, by definition.\n\n\nDerivative\n\\(f'_{a}(x) = \\text{F} \\text{ iff } a=F, 0 \\text{ o.w}\\)\n\n\n\nXOR Gate\n\nTruth Table\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y_{XOR}\\)\n\n\n\n\nT\nT\nF\n\n\nT\nF\nT\n\n\nF\nT\nT\n\n\nF\nF\nF\n\n\n\n\\(XOR(x_1,x_2) \\equiv 0\\) if any of the inputs are 0, by definition.\n\n\nDerivative\n\\(f'_{a}(x) = \\neg a\\)",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bolt/notebooks/dev.html#and-gate-1",
    "href": "bolt/notebooks/dev.html#and-gate-1",
    "title": "Boolean Variation",
    "section": "AND Gate",
    "text": "AND Gate\n\n# check AND gate. Logic and Derivative\n\ndef test_2ary(gate='AND'):\n    A = [-1,1,0]\n    X = [-1,1,0]\n    data = []\n    for element in itertools.product(*[A,X]):\n        a,x = element\n        xb = Bool(x)\n        ab = Bool(a)\n        if gate == 'AND':\n            z = ab*xb\n        elif gate == 'OR':\n            z = ab+xb\n        else:\n            z = ab^xb\n        \n        z.backward()\n        data.append({\n                'a': a,\n                'x': x,\n                'f': z.data,\n                'f\\'': xb.grad\n            })\n\n    df = pd.DataFrame(data)\n    return df\ndf = test_2ary()\nprint(df)\n\n   a  x  f  f'\n0 -1 -1 -1   0\n1 -1  1 -1   0\n2 -1  0  0   0\n3  1 -1 -1   1\n4  1  1  1   1\n5  1  0  0   0\n6  0 -1  0   0\n7  0  1  0   0\n8  0  0  0   0\n\n\nCan see that \\(f'_a(x)\\) is \\(1\\) only when \\(a=1\\), o.w it is 0.",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bolt/notebooks/dev.html#or-gate-1",
    "href": "bolt/notebooks/dev.html#or-gate-1",
    "title": "Boolean Variation",
    "section": "OR Gate",
    "text": "OR Gate\n\n# check AND gate. Logic and Derivative\ndf = test_2ary(gate='OR')\nprint(df)\n\n   a  x  f  f'\n0 -1 -1 -1  -1\n1 -1  1  1  -1\n2 -1  0  0   0\n3  1 -1  1   0\n4  1  1  1   0\n5  1  0  0   0\n6  0 -1  0   0\n7  0  1  0   0\n8  0  0  0   0\n\n\nCan see that \\(f'_a(x)\\) is \\(-1\\) only when \\(a=-1\\), o.w it is 0.",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bolt/notebooks/dev.html#xor-gate-1",
    "href": "bolt/notebooks/dev.html#xor-gate-1",
    "title": "Boolean Variation",
    "section": "XOR Gate",
    "text": "XOR Gate\n\n# check AND gate. Logic and Derivative\ndf = test_2ary(gate='XOR')\nprint(df)\n\n   a  x  f  f'\n0 -1 -1 -1   1\n1 -1  1  1   1\n2 -1  0  0   0\n3  1 -1  1  -1\n4  1  1 -1  -1\n5  1  0  0   0\n6  0 -1  0   0\n7  0  1  0   0\n8  0  0  0   0\n\n\nCan see that \\(f'_a(x)\\) is \\(\\neg a\\) and is 0 when a is 0.",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bolt/notebooks/dev.html#compositions",
    "href": "bolt/notebooks/dev.html#compositions",
    "title": "Boolean Variation",
    "section": "Compositions",
    "text": "Compositions\nChain Rule\nFor \\(\\mathcal{B} \\overset{f}{\\to} \\mathcal{B} \\overset{g}{\\to} \\mathcal{D}\\): \\((g \\circ f)'(x) = \\text{xnor}(g'(f(x)), f'(x))\\), \\(\\forall x \\in \\mathcal{B}\\).\nMultivariate Chain Rule\n\nFor \\(x = (x_1, \\ldots, x_n) \\in \\mathcal{B}^n\\), denote \\(x_{\\neg i} := (x_1, \\ldots, x_{i-1}, \\neg x_i, x_{i+1}, \\ldots, x_n)\\) for \\(n \\ge 1\\) and \\(1 \\leq i \\leq n\\).\nFor \\(f \\in \\mathcal{F}(\\mathcal{B}^n, \\mathcal{B})\\), the (partial) variation of \\(f\\) wrt \\(x_i\\), denoted \\(f'_{i}(x)\\) or \\(\\delta f(x)/\\delta x_i\\), is defined as: \\(f'_{i}(x) \\equiv \\delta f(x)/\\delta x_i \\equiv xnor(\\delta(x_i \\to \\neg x_i), \\delta f(x \\to x_{\\neg i}))\\).\nLet \\(f \\in \\mathcal{F}(\\mathcal{B}^n, \\mathcal{B})\\), \\(n \\geq 1\\), and \\(g \\in \\mathcal{F}(\\mathcal{B}, \\mathcal{B})\\). For \\(1 \\le i \\le n\\),\n\n\\[(g \\circ f)'_i(x) = \\text{xnor}(g'(f(x)), f'_i(x)), \\quad \\forall x \\in \\mathcal{B}^n\\]\n\n\ndef test_Kary(expression = \"x1*x2\", K=2):\n    T = [-1, 1, 0]\n   \n    # Create the Cartesian product of T repeated K times\n    cartesian_product = list(itertools.product(T, repeat=K))\n   \n    # Generate variable names based on the index\n    variable_names = [f'x{i+1}' for i in range(K)]\n\n   \n    data = []\n    # Print the result with variable names and evaluate the expression\n    for product in cartesian_product:\n        # Create a dictionary of variable names and their corresponding values\n        variables = {variable_names[i]: Bool(product[i]) for i in range(K)}\n        \n        # Evaluate the expression using the variables\n        result = eval(expression, {}, variables)\n        result.backward()\n\n        tmp = variables\n        tmp['f: '+ expression] = result.data\n        tmp['f\\'(x1)'] = variables['x1'].grad\n\n        data.append(tmp)\n    \n    df = pd.DataFrame(data)\n    return df\n\nexpression = \"x1 * x2\"\nK = 2\ndf = test_Kary(expression=expression, K=K)\nprint(df)\n\n                x1               x2  f: x1 * x2  f'(x1)\n0  data:-1, grad:0  data:-1, grad:0          -1       0\n1  data:-1, grad:1   data:1, grad:0          -1       1\n2  data:-1, grad:0   data:0, grad:0           0       0\n3   data:1, grad:0  data:-1, grad:1          -1       0\n4   data:1, grad:1   data:1, grad:1           1       1\n5   data:1, grad:0   data:0, grad:0           0       0\n6   data:0, grad:0  data:-1, grad:0           0       0\n7   data:0, grad:0   data:1, grad:0           0       0\n8   data:0, grad:0   data:0, grad:0           0       0\n\n\n\nexpression = \"x1^x2\"\nK = 2\ndf = test_Kary(expression=expression, K=K)\nprint(df)\n\n                 x1                x2  f: x1^x2  f'(x1)\n0   data:-1, grad:1   data:-1, grad:1        -1       1\n1  data:-1, grad:-1    data:1, grad:1         1      -1\n2   data:-1, grad:0    data:0, grad:0         0       0\n3    data:1, grad:1  data:-1, grad:-1         1       1\n4   data:1, grad:-1   data:1, grad:-1        -1      -1\n5    data:1, grad:0    data:0, grad:0         0       0\n6    data:0, grad:0   data:-1, grad:0         0       0\n7    data:0, grad:0    data:1, grad:0         0       0\n8    data:0, grad:0    data:0, grad:0         0       0\n\n\nCan see \\(f_a'(x) = \\neg a\\) and zero whenever any element is 0\nPermutation Invariance\nabc = cbc a+b+c = c+a+b\n\nK = 3\nexpression = \"x1 * x2 * x3\"\ndf1= test_Kary(expression=expression, K=K)\nexpression = \"x3 * x2 * x1\"\ndf2= test_Kary(expression=expression, K=K)\n\n\nprint(np.all(df1[df1.columns[-1]]==df1[df1.columns[-1]]))\nprint(np.all(df1[df1.columns[-2]]==df2[df2.columns[-2]]))\n\nexpression = \"x1 + x2 + x3\"\ndf1= test_Kary(expression=expression, K=K)\n\nexpression = \"x3 + x2 + x1\"\ndf2= test_Kary(expression=expression, K=K)\n\nprint(np.all(df1[df1.columns[-1]]==df1[df1.columns[-1]]))\nprint(np.all(df1[df1.columns[-2]]==df2[df2.columns[-2]]))\n\nTrue\nTrue\nTrue\nTrue\n\n\n\n# check composition\na = Bool(-1);b = Bool(1);c=Bool(1)\nz = b^a+c\nz.backward()\nprint('xor(a,b)\\n', z,a,b,c)\n\nxor(a,b)\n data:-1, grad:1 data:-1, grad:0 data:1, grad:-1 data:1, grad:1\n\n\n\n# check composition\nfrom engine import Bool\nfrom utils import draw_dot\n\n\ny = Bool(-1);\nx1 = Bool(1); x2 = Bool(1)\nw1 = Bool(-1); w2 = Bool(-1); \n\nh1 = x1^w1\nh2 = x2^w2\n\nz = h1*h2\nL = z^y\n\nL.backward()\n\ndraw_dot(L)",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Compressive Learning",
    "section": "",
    "text": "Methods and tools for Compressive Learning.\nCheck BoolGrad, a proof-of-concept implementation of BackProp on Boolean Network, based on BOLD paper, and micrograd from Andrej Karpathy. It is a work in progress and lot of testing is needed.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Compressive Learning</span>"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Motivation",
    "section": "",
    "text": "Why Compressive Learning?\nObservation:\nWe are taking extremely large pre-trained networks (LLMs) and then quantizing them to 1-bit precision. Effectively turning them to Boolean Networks and yet notice that performance does not degrade as much. See 1-bit LLMs paper for recent results. It begets new questions like Can we not train them in the quantized space itself? Answering it requires revising and revisiting many things we learnt over last few decades. We can put the topics into few broad categories.\n\nAnalysis\n\nThe Lottery Ticket Hypothesis\nLottery Ticket Hypothesis(LTH) conjectured that some sub networks are as good as the full network and remaining ones can be completely pruned-off without incurring lot of degradation in performance.\nIn the 1-bit LLMs paper, authors showed that, all weights can just come from \\(\\{ -1,0,1\\}\\). It is the extreme form of quantization and pruning combined. We see such extreme form of quantization of many modern LLMs. This is done for computational efficiency (also energy efficiency) at inference time.\nLTH authors observed that initialization mattered though, in the sense that, same winning/retained sub network retrained from a different initialization did not achieve the same performance the sub network achieved with the the original initialization. I suspect that it is an artefact of the training (learning) algorithm (and not due to the initialization itself).\nWhat role are the prune-able sub networks playing? Is that about redundancy (serving some other important function such as a error control coding) or they are just plain useless. It is not clear unless we investigate in that direction. The LTH paper did not study the brittleness after pruning (AFIK).\nAre the pruned networks acting like “error control codes” in the functional space? Not having them is ok from a prediction (accuracy) point of view , but not ok from robustness (sensitivity) point of view.\nHow we do study generalization ability of such binary networks? From a build point of view, what is needed to generalize? I hypothesize that smoothness of the functions is central to generalization. So, how do we define and later enforce such smoothness on Boolean Networks.\nA related question - what is the equivalent notion of robustness for Boolean maps?\nHow do we analyze (and develop the theory to analyze) such Boolean functions which map n-dimensional inputs bit streams to m-dimensional output bit streams? Will the theories of error control codes based on groups and fields help?\n\n\n\nBuild & Develop\n\nDo we need to go through the route of Train large models with full precision and quantize or can we Learn in the 1-bit space itself. Drawing inspiration from compressive sensing, can we do Compressive Learning?.\nHow do we train such 1-bit models to begin with? If autograd and backprop made possible the training (with non-trivial enablers like data, compute and massive engineering feats at data center scale) - are there efficient learning algos to train 1-bit LLMs?\nBoolean Logic Deep Learning (BOLD) introduced some Variational Theory to define gradients on Boolean variables, and a corresponding chain rule to go with.\nThe implication is that - we can compute the gradients on mixed data types on any computational DAG (all neural networks are instances of such graphs) via backprop algo. But will it work at scale?\n\n\n\nKernel Machines\nCan Kernels bridge the digital (Boolean Networks) and the analog (current full precision LLMs) divide?\nBefore Deep Learning took-off circa 2012, most successful models were Kernel Machines. It was a well researched area, and probably well understood as well, relative to Deep Learning. Some evidence that they can.\n\nTrees are Kernels Machines: Some might argue - what about Trees and Ensembles? They are not Kernel Machines. Under the hood, Random Forests are also Kernel Machines. In an obscure paper Some infinity theory for predictor ensembles, Leo Brieman shows that Random Forests implicitly learn a Laplacian Kernel, asymptotically.\nTransformers are Kernel Machines: In another insightful work, Prof. Rich B who did pioneering work in signal processing in general, and compressive sensing in particular (the 1-pixel camera project, for example), re-derived Transformers, like Prof. Yi Ma’s group, but using a different route - by looking at Transformers as Kernel Machines. In A Primal-Dual Framework for Transformers and Neural Networks, an explicit construction of a polynomial kernel was given, whose dual form gives raise to the self-attention block of Transformers. Perhaps this can explain why Transformers were not very competitive to Trees (due to the different inductive biases encoded by their respective Kernels. Not all Kernels are equal).\nSGD is a Kernel Machine Learner: In this paper, Prof. Pedro Domingos shows that every model learned by Gradient Descent is approximately a Kernel Machine. The Kernel happens to be Neural Tangent Kernel (NTK), which seems to be main analytical device to study the behavior of Deep Neural Networks (See this for example).\nRandom Features are Large Scale Kernel Machines: In a time-tested paper, Ali and Ben, show that linear models on random features are Kernel Machines, and they are extremely competitive and simple (to implement and model), at the same time.\nSeparability in High Dimensional Feature space: The conventional wisdom in ML is, transform low dim input to very high (non linear) features space, where modeling becomes easier. Kernels provide that kind of Statistical Machinery to both compute and theorize the development of new models (architectures) and learning algorithms.\n\nBut what about Boolean functions, i.e, functions that map n-dimensional Boolean inputs to m-dimensional Boolean outputs? In this beautiful paper Learning of Boolean Functions Using Support Vector Machines, publised in 2001, Ken Sadohara develops DNF Kernels to be applied along side SVMs. The connection between DNF (Disjunctive Normal Form) and Boolean circuits are rather straightforward to see. Any truth table (coding a Boolean function) can be represented either in Sum-of-Products (SOP) or Product-of-Sum (POS) form, and DNF corresponds to the SOP form. So, if we can construct a model and learn algorithms to go with, such model becomes a universal learner (of any Boolean function). This is the constructionist approach to model(architecture) design. Using this constructionist approach, we can argue that Boolean SOP circuits are MLPs in the Boolean space, see here for discussion.\nSo the thesis is - All modern, empirically proven architectures are a Composition of Learnable Kernel Machines and perhaps we can extend this to Deep Binary Neural Networks as well.\n\n\nAxiomatic Notions of Intelligence\n\nPrinciple of Parsimony and Self-Consistency: In a series of papers, based on decades of work, Prof. Yi Ma and colleagues argued that for intelligence to emerge, certain aspects must be considered such as representations must be parsimonious (not exactly sparse) and they must be self-consistent (to recover the original signal).\nCompression: A key point Prof. Yi Ma emphasizes is that Compression is at the heart of intelligence. Using rate-reduction theory (widely used in communication theory), his group was able to re-derive Transformers, ConvNets, the empirically proven architectures from first principles.\nAbility to learn based on few examples: Intelligent systems should be able to learn based on the feedback loop (error control) with few examples.\nRecollection is not a sign of intelligence: By that account, taking a cue from Prof. Subbarao Khambampati, LLMs are approximate retrieval machines, which do not have any reasoning ability, unless specifically equipped to do so, as the recent works seem to suggest.\n\n\n\nAxiomatic Notions of Perception and Processing\n\nPerceive in low dimensions but with high precision (fidelity).\nProject them on to very high dimensional, parsimonious, self-consistent representations but in low precision (1 bit for eg)\nProcess them in the bit space (energy efficient)\n\nWhat it means is, the interface between the external world and the processor (reasoner or model) is like an A/D Convertor (Analog to Digital converter) and the processor (model) only performs bitwise operations and we can convert the Binary signals back to Analog (D/A converter) for external communication.\nPutting it all together,\n\n\nCompressive Learning\nis to study and develop\n\nComputational Training Stack\n\ndefine gradients on Boolean variables, and a chain rule to go with\ndevelop backprop to scale to large computational DAGs\nPerhaps, use Genetic Algorithms to augment the training\n\nKernels\n\nLearnable DNF Kernels to learn Boolean features or other Universal Learners of Boolean N/Ws in SOP or POS form\nCompose the Kernels depth-wise to retain the expressivity of modern Deep Learning models.\n\nIssues\n\nCan they be trained? Or do they just rote learn?\nCan they generalize?\nCan they be distilled?\nCan they be analyzed?\n\n\nIf successful, an optimistic outlook for these Deep Binary Neural Networks is, they are:\n\nInterpretable: since we can recover the DNF forms\nModulo-LLM-ready: since symbols can be mined explicitly, can augment symbolic reasoning, making the LLMs modulo-LLMs\nEnergy-efficient: require bit-wise operations, not needing giant matrix multiplications (so no GPUs)\nASIC-friendly: we may be able compile PyTorch models directly into HDL languages and burn the models on silicon. Like 3D printing, forge your model on silicon.\nSLA-friendly: with high token throughput and low latency\nEdge-friendly: with low memory footprint, can be deployed on edge devices\nEnd-to-End trainable: no need for train-large-then-quantize paradigm - train in the compressed domain itself.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Motivation</span>"
    ]
  },
  {
    "objectID": "bold.html",
    "href": "bold.html",
    "title": "Boolean Variation",
    "section": "",
    "text": "Definition",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bold.html#definition",
    "href": "bold.html#definition",
    "title": "Boolean Variation",
    "section": "",
    "text": "Three-value and Mixed logic\n\nDefine \\(\\mathcal{M} \\equiv \\mathcal{B} \\cup \\{0\\}\\) with logic connectives defined according to those of Boolean logic as follows.\n\nFirst, the negation is: \\(\\neg True = False\\), \\(\\neg False = True\\), and \\(\\neg 0 = 0\\).\nSecond, let \\(\\mathcal{L}\\) be a logic connective, denote by \\(\\mathcal{L}_{\\mathcal{M}}\\) and \\(\\mathcal{L}_{\\mathcal{B}}\\) when it is in \\(\\mathcal{M}\\) and in \\(\\mathcal{B}\\), respectively, then \\(\\mathcal{L}_{\\mathcal{Mb}}(a,b) = \\mathcal{L}_{\\mathcal{Bb}}(a,b)\\) for \\(a, b \\in \\mathcal{B}\\) and \\(\\mathcal{L}_{\\mathcal{M}}(a,b) = 0\\) otherwise.\n\nDenote by \\(\\mathcal{L}\\) a logic set (e.g., \\(\\mathcal{B}\\) or \\(\\mathcal{M}\\)), \\(\\mathcal{R}\\) the real set, \\(\\mathcal{Z}\\) the set of integers, \\(\\mathcal{N}\\) a numeric set (e.g., \\(\\mathcal{R}\\) or \\(\\mathcal{Z}\\)), and \\(\\mathcal{D}\\) a certain set of \\(\\mathcal{L}\\) or \\(\\mathcal{N}\\).\nFor \\(x \\in \\mathcal{N}\\), its logic value denoted by \\(x_{logic}\\) is given as \\(x_{logic} = True \\Leftrightarrow x &gt; 0\\), \\(x_{logic} = False \\Leftrightarrow x &lt; 0\\), and \\(x_{logic} = 0 \\Leftrightarrow x = 0\\).\nThe magnitude of a variable \\(x\\), denoted \\(|x|\\), is defined as its usual absolute value if \\(x \\in \\mathcal{N}\\). And for \\(x \\in \\mathcal{L}\\): \\(|x| = 0\\) if \\(x = 0\\), and \\(|x| = 1\\) otherwise.\nFor \\(\\mathcal{L}\\) a logic connective of \\(\\mathcal{L}\\) and variables \\(a\\), \\(b\\), operation \\(c = \\mathcal{L}(a, b)\\) is defined such that \\(|c| = |a||b|\\) and \\(c_{logic} = \\mathcal{L}(a_{logic}, b_{logic})\\).\n\n\n\nCalculus\n\nA variable x is (extended) Boolean variable with the following encoding \\(x \\in \\{-1,0,1\\}\\) where \\(-1\\) represents logical \\(False\\), \\(+1\\) represents logical \\(True\\), and \\(0\\) represents \\(Ignore\\). We can call this extended Boolean domain.\nOrder relations \\(&lt;\\) and \\(&gt;\\) in \\(\\mathcal{B}\\) are defined as follows: \\(False &lt; True\\), and \\(True &gt; False\\).\nFor \\(a, b \\in \\mathcal{B}\\), the variation from \\(a\\) to \\(b\\), denoted \\(\\delta(a \\to b)\\), is defined as: \\(\\delta(a \\to b) \\equiv True\\) if \\(b &gt; a\\), \\(\\equiv 0\\) if \\(b = a\\), and \\(\\equiv False\\) if \\(b &lt; a\\).\nFor \\(f \\in \\mathcal{F}(\\mathcal{B}, \\mathcal{D})\\), \\(\\forall x \\in \\mathcal{B}\\), write \\(\\delta f(x \\to \\neg x) := \\delta(f(x) \\to f(\\neg x))\\). The variation of \\(f\\) w.r.t \\(x\\), denoted \\(f'(x)\\), is defined as: \\(f'(x) \\equiv \\text{xnor}(\\delta(x \\to \\neg x), \\delta f(x \\to \\neg x))\\).\n\nFor simplicity, we will write \\(\\delta f\\) to denote \\(\\delta f(x \\to \\neg x)\\). Similarly, \\(\\delta x\\) to denote \\(\\delta (x \\to \\neg x)\\)\nFor details see, Section 3.2 of BOLD: Boolean Logic Deep Learning",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bold.html#truth-tables-for-basic-gates",
    "href": "bold.html#truth-tables-for-basic-gates",
    "title": "Boolean Variation",
    "section": "Truth Tables for basic gates",
    "text": "Truth Tables for basic gates\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y_{AND}\\)\n\\(y_{OR}\\)\n\\(y_{XOR}\\)\n\\(y_{XNOR}\\)\n\n\n\n\nT\nT\nT\nT\nF\nT\n\n\nT\nF\nF\nT\nT\nF\n\n\nF\nT\nF\nT\nT\nF\n\n\nF\nF\nF\nF\nF\nT\n\n\n\nFor any gate, if any of the inputs is \\(Ignore\\), its output is also \\(Ignore\\), by definition.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bold.html#derivatives",
    "href": "bold.html#derivatives",
    "title": "Boolean Variation",
    "section": "Derivatives",
    "text": "Derivatives\nRecall:\n\n\\(\\delta(a \\to b) \\equiv True\\) if \\(b &gt; a\\), \\(\\equiv 0\\) if \\(b = a\\), and \\(\\equiv False\\) if \\(b &lt; a\\).\n\\(f'(x) \\equiv \\text{xnor}(\\delta(x \\to \\neg x), \\delta f(x \\to \\neg x))\\).\n\n\nAND Gate\nThe Truth Table for \\(f'(x) = f'_a(x)\\) for \\(f_a(x) = AND(x,a)\\) is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(a\\)\n\\(x\\)\n\\(\\neg x\\)\n\\(\\delta x\\)\n\\(f(x)\\)\n\\(f(\\neg x)\\)\n\\(\\delta f\\)\n\\(f'\\)\n\n\n\n\nT\nT\nF\nF\nT\nF\nF\nT\n\n\nT\nF\nT\nT\nF\nT\nT\nT\n\n\nF\nT\nF\nF\nF\nF\n0\n0\n\n\nF\nF\nT\nT\nF\nF\n0\n0\n\n\n\nTherefore, \\(f'_{a}(x) = \\text{T} \\text{ iff } a=T, 0 \\text{ o.w}\\)\n\n\nOR Gate\nThe Truth Table for \\(f'(x) = f'_a(x)\\) for \\(f_a(x) = OR(x,a)\\) is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(a\\)\n\\(x\\)\n\\(\\neg x\\)\n\\(\\delta x\\)\n\\(f(x)\\)\n\\(f(\\neg x)\\)\n\\(\\delta f\\)\n\\(f'\\)\n\n\n\n\nT\nT\nF\nF\nT\nT\n0\n0\n\n\nT\nF\nT\nT\nT\nT\n0\n0\n\n\nF\nT\nF\nF\nT\nF\nF\nF\n\n\nF\nF\nT\nT\nF\nT\nT\nF\n\n\n\nTherefore, \\(f'_{a}(x) = \\text{F} \\text{ iff } a=F, 0 \\text{ o.w}\\)\n\n\nXOR Gate\nThe Truth Table for \\(f'(x) = f'_a(x)\\) for \\(f_a(x) = XOR(x,a)\\) is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(a\\)\n\\(x\\)\n\\(\\neg x\\)\n\\(\\delta x\\)\n\\(f(x)\\)\n\\(f(\\neg x)\\)\n\\(\\delta f\\)\n\\(f'\\)\n\n\n\n\nT\nT\nF\nF\nF\nT\nT\nF\n\n\nT\nF\nT\nT\nT\nF\nF\nF\n\n\nF\nT\nF\nF\nT\nF\nF\nT\n\n\nF\nF\nT\nT\nF\nT\nT\nT\n\n\n\nTherefore, \\(f'_{a}(x) = \\neg a\\)\n\n\nXNOR Gate\nThe Truth Table for \\(f'(x) = f'_a(x)\\) for \\(f_a(x) = XNOR(x,a)\\) is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(a\\)\n\\(x\\)\n\\(\\neg x\\)\n\\(\\delta x\\)\n\\(f(x)\\)\n\\(f(\\neg x)\\)\n\\(\\delta f\\)\n\\(f'\\)\n\n\n\n\nT\nT\nF\nF\nT\nF\nF\nT\n\n\nT\nF\nT\nT\nF\nT\nT\nT\n\n\nF\nT\nF\nF\nF\nT\nT\nF\n\n\nF\nF\nT\nT\nT\nF\nF\nF\n\n\n\nTherefore, \\(f'_{a}(x) = a\\)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bold.html#compositions",
    "href": "bold.html#compositions",
    "title": "Boolean Variation",
    "section": "Compositions",
    "text": "Compositions\nChain Rule\nFor \\(\\mathcal{B} \\overset{f}{\\to} \\mathcal{B} \\overset{g}{\\to} \\mathcal{D}\\): \\((g \\circ f)'(x) = \\text{xnor}(g'(f(x)), f'(x))\\), \\(\\forall x \\in \\mathcal{B}\\).\nMultivariate Chain Rule\n\nFor \\(x = (x_1, \\ldots, x_n) \\in \\mathcal{B}^n\\), denote \\(x_{\\neg i} := (x_1, \\ldots, x_{i-1}, \\neg x_i, x_{i+1}, \\ldots, x_n)\\) for \\(n \\ge 1\\) and \\(1 \\leq i \\leq n\\).\nFor \\(f \\in \\mathcal{F}(\\mathcal{B}^n, \\mathcal{B})\\), the (partial) variation of \\(f\\) wrt \\(x_i\\), denoted \\(f'_{i}(x)\\) or \\(\\delta f(x)/\\delta x_i\\), is defined as: \\(f'_{i}(x) \\equiv \\delta f(x)/\\delta x_i \\equiv \\text{xnor}(\\delta(x_i \\to \\neg x_i), \\delta f(x \\to x_{\\neg i}))\\).\nLet \\(f \\in \\mathcal{F}(\\mathcal{B}^n, \\mathcal{B})\\), \\(n \\geq 1\\), and \\(g \\in \\mathcal{F}(\\mathcal{B}, \\mathcal{B})\\). For \\(1 \\le i \\le n\\),\n\n\\[(g \\circ f)'_i(x) = \\text{xnor}(g'(f(x)), f'_i(x)), \\quad \\forall x \\in \\mathcal{B}^n\\]",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bold.html#n-ary-gates",
    "href": "bold.html#n-ary-gates",
    "title": "Boolean Variation",
    "section": "N-ary gates",
    "text": "N-ary gates\nWe will represent \\(f(x)\\) to represent multivariate Boolean function, and \\(f(x_{-i})\\) to mean, compute the function by excluding the i-th variable. Let \\(H(.)\\) represent the Heavyside step function, defined as \\(H(x)= 1\\) if \\(x &gt; 0\\) and 0 otherwise.\nN-ary AND gate \\[\n\\begin{array}{left}\nf(x) &=& \\land_{j=1}^{n} x_j \\\\\nf'_{x_{-i}}(x_i) &=& T \\text{ if } f(x_{-i})=T \\\\\n&=& 0 \\text{ o.w }\n\\end{array}\n\\] Therefore\n\\(\\frac{\\partial{f(x)}}{\\partial{x_i}} = H(f(x_{-i}))\\).\nN-ary OR gate \\[\n\\begin{array}{left}\nf(x) &=& \\lor_{j=1}^{n} x_j \\\\\nf'_{x_{-i}}(x_i) &=& F \\text{ if } f(x_{-i})=F \\\\\n&=& 0 \\text{ o.w }\n\\end{array}\n\\] Therefore\n\\(\\frac{\\partial{f(x)}}{\\partial{x_i}} = -H(-f(x_{-i}))\\).\nN-ary XOR gate \\[\n\\begin{array}{left}\nf(x) &=& \\oplus_{j=1}^{n} x_j \\\\\nf'_{x_{-i}}(x_i) &=& \\neg f(x_{-i})\n\\end{array}\n\\] Therefore\n\\(\\frac{\\partial{f(x)}}{\\partial{x_i}} = -f(x_{-i})\\).\nN-ary XNOR gate \\[\n\\begin{array}{left}\nf(x) &=& \\ominus_{j=1}^{n} x_j \\\\\nf'_{x_{-i}}(x_i) &=& f(x_{-i})\n\\end{array}\n\\] Therefore\n\\(\\frac{\\partial{f(x)}}{\\partial{x_i}} = f(x_{-i})\\).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bnn_layer.html",
    "href": "bnn_layer.html",
    "title": "BNN Layer",
    "section": "",
    "text": "Sum of Product Networks\nConsider a three variable multi-valued Truth table.\nFor an n-variable Boolean function, the Truth Table will have \\(2^n\\) unique rows, and the size of all Boolean functions is \\(2^{2^n}\\). Above, we considered \\(y_1, y_2, y_3\\) for illustration.\nWe can model \\[\n\\begin{array}{left}\ny_1 &=& \\neg x_1 \\neg x_2 \\neg x_3 \\lor \\neg x_1 \\neg x_2 x_3\\\\\ny_2 &=& \\neg x_1 \\neg x_2 x_3 \\lor  \\neg x_1 x_2 \\neg x_3 \\lor x_1 \\neg x_2 \\neg x_3 \\lor x_1 x_2 x_3\\\\\ny_3 &=&  x_1 x_2 x_3\n\\end{array}\n\\]\nWe notice any Truth Table (\\(y_1\\), for example) can be expressed in Sum-of-Products form. The innards are the product terms which are to be OR-ed.\nGenerally speaking, we model the Truth Tables via learnable gates of n-variables as follows:\n\\[\n\\begin{array}{left}\nh_j &=& \\land_{i=1}^{n} x_i \\oplus w_i^j \\\\\ny_k &=& \\uplus_{j=1}^{m} \\alpha_{j}^{k} \\odot h_j \\text{ where } \\alpha_{j}^{k} \\in \\{0,1\\} \\text{ and } \\exists j \\text{ s.t }  \\alpha_{j}^{k} = 1 \\forall k\n\\end{array}\n\\] Above, \\(\\alpha_j^k\\) is a selection gate. When all of them are cold (off), model will be in an \\(Ignore\\) state. The Truth Table for the selection gate \\(h_j \\otimes \\alpha_j\\) is:\nAnd, \\(\\uplus(.)\\) is an modified OR gate works like regular OR gate when at least one of the inputs is not in \\(Ignore\\) state, which is different from the OR gate we have defined earlier. The Truth Table for an N-ary SelectOR gate is defined as follows:\nTherefore, \\[\n\\begin{array}{left}\n\\uplus(x,a) &=& x \\text{ if }  a=0 \\\\\n&=&  a \\text{ if }  x=0 \\\\\n&=&  \\lor (x,a) \\text{ o.w }\n\\end{array}\n\\]\nFurther, \\[\n\\begin{array}{left}\nx \\oplus w &=& \\neg x \\text{ if } w = T \\\\\n&=&  x \\text{  o.w }\n\\end{array}\n\\] In effect, we are using XOR gate to complement the inputs, which is essential to create the product terms and using the SelectOR gate to select the prod terms and OR them.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>BNN Layer</span>"
    ]
  },
  {
    "objectID": "bnn_layer.html#sum-of-product-networks",
    "href": "bnn_layer.html#sum-of-product-networks",
    "title": "BNN Layer",
    "section": "",
    "text": "\\(x_1\\)\n\\(x_2\\)\n\\(x_3\\)\n\\(y_1\\)\n\\(y_2\\)\n\\(y_3\\)\nProd Terms\n\n\n\n\nF\nF\nF\nT\nF\nF\n\\(h_1 = \\neg x_1 \\neg x_2 \\neg x_3\\)\n\n\nF\nF\nT\nT\nT\nF\n\\(h_2 = \\neg x_1 \\neg x_2 x_3\\)\n\n\nF\nT\nF\nF\nT\nF\n\\(h_3 = \\neg x_1 x_2 \\neg x_3\\)\n\n\nF\nT\nT\nF\nF\nT\n\\(h_4 = \\neg x_1 x_2  x_3\\)\n\n\nT\nF\nF\nF\nT\nF\n\\(h_5 = x_1 \\neg x_2 \\neg x_3\\)\n\n\nT\nF\nT\nF\nF\nF\n\\(h_6 = x_1 \\neg x_2 x_3\\)\n\n\nT\nT\nF\nF\nF\nF\n\\(h_7 = x_1 x_2 \\neg x_3\\)\n\n\nT\nT\nT\nF\nT\nT\n\\(h_8 =  x_1 x_2  x_3\\)\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha_j\\)\n\\(h_j\\)\n\\(\\alpha_j \\odot h_j\\)\n\n\n\n\nT\nT\nT\n\n\nT\nF\nF\n\n\nF\nT\n0\n\n\nF\nF\n0\n\n\n\n\n\n\n\n\\(a\\)\n\\(x\\)\n\\(\\uplus(x,a)\\)\n\n\n\n\nT\nT\nT\n\n\nT\nF\nT\n\n\nF\nT\nT\n\n\nF\nF\nF\n\n\n0\nT\nT\n\n\n0\nF\nF\n\n\n0\n0\n0\n\n\nT\n0\nT\n\n\nF\n0\nF",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>BNN Layer</span>"
    ]
  },
  {
    "objectID": "bnn_layer.html#prodtron",
    "href": "bnn_layer.html#prodtron",
    "title": "BNN Layer",
    "section": "ProdTron",
    "text": "ProdTron\nWe want to realize a module which takes n-inputs and creates a product term, i.e., we want to realize \\[\n\\begin{array}{left}\nh(x,w) &=& \\land_{i=1}^{n} x_i \\oplus w_i\n\\end{array}\n\\]\nFor convenience, we write them recursively, to ease up the derivation, \\(h= \\land(x_i \\oplus w_i, h_{-i})\\), where \\(h_{-i} = \\land_{k \\neq i} x_k \\oplus w_k\\)\nWe obtain the partial derivatives as: \\[\n\\begin{array}{left}\n\\frac{\\partial{h(x,w)}}{\\partial{x_i}} &=& w_i \\text{ if } h_{-i} = T\\\\\n&=& 0 \\text{ o.w } \\\\\n\\frac{\\partial{h(x,w)}}{\\partial{w_i}} &=& x_i \\text{ if } h_{-i} = T\\\\\n&=& 0 \\text{ o.w } \\\\\n\\end{array}\n\\]\nIt can be implemented with Heavyside function as follows: \\[\n\\begin{array}{left}\n\\frac{\\partial{h(x,w)}}{\\partial{x_i}} &=&  w_i H(h_{-i}) \\\\\n\\frac{\\partial{h(x,w)}}{\\partial{w_i}} &=& x_i H(h_{-i})\n\\end{array}\n\\] where \\(\\epsilon\\) is a small positive constant.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>BNN Layer</span>"
    ]
  },
  {
    "objectID": "bnn_layer.html#sumtron",
    "href": "bnn_layer.html#sumtron",
    "title": "BNN Layer",
    "section": "SumTron",
    "text": "SumTron\nWe want to realize a module which takes m-inputs, selects some of the m-inputs, and OR’s them. \\[\n\\begin{array}{left}\ny(\\alpha, h) &=& \\uplus_{j=1}^{m} \\alpha_j \\odot h_j\n\\end{array}\n\\] For convenience, we write them recursively, to ease up the derivation, \\(y = \\uplus(\\alpha_j \\odot h_j, y_{-j})\\), where \\(y_{-j} = \\uplus_{k=1, \\neq j  }^{m} \\alpha_k \\odot h_k\\). We can see that \\(y = y_{-j} \\text { if } \\alpha_j = F\\)\nWe obtain the partial derivatives by observing the Truth Table to get \\(\\frac{\\partial{y(\\alpha,h)}}{\\partial{h_i}}\\) as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha_j\\)\n\\(h_j\\)\n\\(y_{-j}\\)\n\\(y\\)\n\\(y(\\neg h_j)\\)\n\\(\\delta y\\)\n\\(\\delta h_j\\)\n\\(y'\\)\n\n\n\n\nT\nT\nT/F/0\nT\nT/F/F\n0/F/F\nF\n0/T/T\n\n\nT\nF\nT/F/0\nT/F/F\nT\n0/T/T\nT\n0/T/T\n\n\nF\nT\nT/F/0\nT/F/0\nT/F/0\n0\nF\n0\n\n\nF\nF\nT/F/0\nT/F/0\nT/F/0\n0\nF\n0\n\n\n\nTherefore,\n\\[\n\\begin{array}{left}\n\\frac{\\partial{y(\\alpha,h)}}{\\partial{h_i}} &=&  T \\text{ if } \\alpha_j = T\\, \\& \\, y_{-j} = F/0 \\\\\n&=& 0 \\text{ o.w } \\\\\n\\end{array}\n\\] It can be implemented with Heavyside function as follows: \\[\n\\begin{array}{left}\n\\frac{\\partial{y(\\alpha,h)}}{\\partial{h_j}} &=&  H( \\alpha_j)H(-y_{-j} + \\epsilon)\n\\end{array}\n\\] where \\(\\epsilon\\) is a small positive constant.\nWe obtain the partial derivatives by observing the Truth Table to get \\(\\frac{\\partial{y(\\alpha,h)}}{\\partial{\\alpha_i}}\\) as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha_j\\)\n\\(h_j\\)\n\\(y_{-j}\\)\n\\(y\\)\n\\(y(\\neg \\alpha_j)\\)\n\\(\\delta y\\)\n\\(\\delta \\alpha_j\\)\n\\(y'\\)\n\n\n\n\nT\nT\nT/F/0\nT\n\\(y_{-j}\\)\n0\nF\n0\n\n\nT\nF\nT/F/0\nT/F/F\n\\(y_{-j}\\)\n0\nF\n0\n\n\nF\nT\nT/F/0\n\\(y_{-j}\\)\nT\n0/T/0\nT\n0/T/0\n\n\nF\nF\nT/F/0\n\\(y_{-j}\\)\nT/F/F\n0/0/0\nT\n0\n\n\n\nTherefore,\n\\[\n\\begin{array}{left}\n\\frac{\\partial{y(\\alpha,h)}}{\\partial{\\alpha_j}} &=&  T \\text{ if } \\alpha_j = F\\, \\& \\, y_{-j} = F \\, \\&  \\, h_j = T \\\\\n&=& 0 \\text{ o.w } \\\\\n\\end{array}\n\\] It can be implemented with Heavyside function as follows: \\[\n\\begin{array}{left}\n\\frac{\\partial{y(\\alpha,h)}}{\\partial{h_i}} &=&  H(\\alpha_j)H(-y_{-j})H(h_{j})\n\\end{array}\n\\] where \\(\\epsilon\\) is a small positive constant.\nFinally, we can put it all together",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>BNN Layer</span>"
    ]
  },
  {
    "objectID": "bnn_layer.html#bnn-layer",
    "href": "bnn_layer.html#bnn-layer",
    "title": "BNN Layer",
    "section": "BNN Layer",
    "text": "BNN Layer\nA BNN Layer is a Boolean Neural Network that maps an N-dimensional input to an K-dimension output, with a hyper parameter \\(H\\) that controls the layer complexity, resulting in a total of \\(H(N+K)\\) learnable Boolean weights.\n\\[\n\\begin{array}{left}\ny_{k} &=& \\uplus_{j=1}^{m} \\alpha_{j}^{k} \\odot \\left( h_j \\equiv \\land_{i=1}^{n} x_i \\oplus w_i^j \\right)\n\\end{array}\n\\] where \\(n=1,\\dots,N\\), \\(j=1,\\dots,H\\) \\(k=1,\\dots,K\\).\nWe can revisit the example again and see that the following weights will realize the respective Truth Values. To model \\(y_1 = h_1 \\lor h_2\\), we need\n\\[\n\\begin{array}{left}\n\\alpha_{j}^1 &=& T \\text{ for } j = 1,2 \\text{  and } F \\text{ o.w}\\\\\nw_{i}^1 &=& T \\text{ for } j \\in {1,2,3} \\text{ corresponding to }  h_1  \\\\\nw_{i}^2 &=& T  \\text{ for } j \\in {1,2}  \\text{  and } F \\text{ o.w corresponding to } h_2\n\\end{array}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>BNN Layer</span>"
    ]
  },
  {
    "objectID": "sparse_bnn_layer.html",
    "href": "sparse_bnn_layer.html",
    "title": "Sparse BNN Layer",
    "section": "",
    "text": "\\(\\oplus\\) Flip Block",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sparse BNN Layer</span>"
    ]
  },
  {
    "objectID": "sparse_bnn_layer.html#oplus-flip-block",
    "href": "sparse_bnn_layer.html#oplus-flip-block",
    "title": "Sparse BNN Layer",
    "section": "",
    "text": "Logic: Flip the inputs\n\\[\nx \\oplus w =\n\\begin{cases}\n  \\neg x & \\text{ if } w = T \\\\\nx & \\text{  o.w }\n\\end{cases}\n\\] where \\(x,w \\in \\{-1 \\equiv F,1 \\equiv T\\}\\). In effect, we are using XOR gate to complement the inputs, which is essential to create the product terms and using the SelectOR gate (defined later) to select the prod terms and OR them.\n\n\nForward Method\nWe can compute 2-ary Flipper as follows: \\[\n\\begin{array}{left}\nx \\oplus w &=& -w x\n\\end{array}\n\\]\n\n\nBackward Method: Local Gradients\nIn the network, XOR is used to flip the input based on a weight, so it will be a 2-ary. \\[\n\\begin{array}{left}\n\\tilde{x_i} &=& x_i \\oplus w_i \\\\\n\\frac{\\partial \\tilde{x_i}}{\\partial x_i} &=& \\neg w_i \\\\\n\\frac{\\partial \\tilde{x_i}}{\\partial w_i} &=& \\neg x_i\n\\end{array}\n\\] To compute negation, we flip the sign since \\(w,x \\in \\{-1,1\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sparse BNN Layer</span>"
    ]
  },
  {
    "objectID": "sparse_bnn_layer.html#odot-select-block",
    "href": "sparse_bnn_layer.html#odot-select-block",
    "title": "Sparse BNN Layer",
    "section": "\\(\\odot\\) Select Block",
    "text": "\\(\\odot\\) Select Block\n\nLogic: Select a variable, Ignore otherwise\nThe Truth Table for the Select gate is\n\n\n\n\\(\\beta\\)\n\\(x\\)\n\\(\\beta \\odot x\\)\n\n\n\n\nT\nT\nT\n\n\nT\nF\nF\n\n\nF\nT\n0\n\n\nF\nF\n0\n\n\n\nWe will embed them into AND and OR gates, and derive their respective Truth Tables and Gradients. We will not use the Select gate in isolation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sparse BNN Layer</span>"
    ]
  },
  {
    "objectID": "sparse_bnn_layer.html#sqcap-sand-select-and-and-block",
    "href": "sparse_bnn_layer.html#sqcap-sand-select-and-and-block",
    "title": "Sparse BNN Layer",
    "section": "\\(\\sqcap\\) SAND: Select and AND Block",
    "text": "\\(\\sqcap\\) SAND: Select and AND Block\n\nLogic: Select and AND the inputs\nWe want to realize a module which takes n-inputs (some of which are flipped), selects some of the n-inputs, and AND’s them. \\[\n\\begin{array}{left}\nh(\\beta, \\tilde{x}) = \\sqcap_{i=1}^{n} \\beta_i \\odot \\tilde{x}_i\n\\end{array}\n\\]\nFor convenience, we write them recursively, to ease up the derivation, \\(h = \\sqcap (\\beta_i \\odot \\tilde{x}_i, h_{-i})\\), where \\(h_{-i} = \\sqcap_{k=1, \\neq i  }^{n} \\beta_k \\odot \\tilde{x}_k\\). \\[\nh(\\beta, \\tilde{x}) =\n\\begin{cases}\n\\land(\\tilde{x}_{i}, h_{-i}) & \\text{ if }  \\beta_i = T \\land h_i \\in \\{T,F\\} \\\\\n\\tilde{x}_{i} & \\text { if } \\beta_i = T \\text{ and }  h_{-i} = 0 \\\\\nh_{-i} & \\text { if } \\beta_i = F\n\\end{cases}\n\\] This looks much complicated than what it really is trying to say. Rewriting it to get, \\[\nh(\\beta, \\tilde{x}) =\n\\begin{cases}\n0 & \\text{ if } \\beta_i = F \\,\\, \\forall i \\\\\n\\land_k \\tilde{x}_{k} & \\text{ for }  k \\in \\{ i \\text{ s.t } \\beta_i = T\\}\n\\end{cases}\n\\]\n\n\nForward Method\nIt can be obtained with Heavyside function as follows: \\[\nh(\\beta, \\tilde{x}) = H\\left(\\sum_{i=1}^n 0.5(\\beta_i+1) - \\epsilon \\right) \\left(2H\\left(\\sum_{i=1}^n 0.25(\\tilde{x}_i-1)(\\beta_i+1) + \\epsilon \\right)-1 \\right)\n\\]\n\n\nBackward Method: Local Gradients\nWe obtain the partial derivatives by observing the Truth Table to get \\(\\frac{\\partial{y(\\alpha,h)}}{\\partial{\\alpha_i}}\\) as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\beta_i\\)\n\\(\\tilde{x}_i\\)\n\\(h_{-j}\\)\n\\(h\\)\n\\(h(\\neg \\beta_i)\\)\n\\(\\delta h\\)\n\\(\\delta \\alpha_i\\)\n\\(h'\\)\n\n\n\n\nT\nT\nT/F/0\n\\(h_{-i}\\)\n\\(h_{-i}\\)\n0\nF\n0\n\n\nT\nF\nT/F/0\nF\n\\(h_{-i}\\)\nT/0/0\nF\nF/0/0\n\n\nF\nT\nT/F/0\n\\(h_{-i}\\)\n\\(h_{-i}\\)\n0\nT\n0\n\n\nF\nF\nT/F/0\n\\(h_{-i}\\)\nF\nF/0/0\nT\nF/0/0\n\n\n\nTherefore,\n\\[\n\\frac{\\partial{h(\\beta,\\tilde{x})}}{\\partial{\\beta_i}} =\n\\begin{cases}\n  F & \\text{  when } \\tilde{x_i} = F \\land h_i = T  \\\\\n0 & \\text{ o.w }\n\\end{cases}\n\\] It can be implemented with Heavyside function as follows: \\[\n\\begin{array}{left}\n\\frac{\\partial{h(\\alpha,h)}}{\\partial{\\alpha_j}} &=&  H(x_j)H(y_{-i})\n\\end{array}\n\\]\nWe obtain the partial derivatives by observing the Truth Table to get \\(\\frac{\\partial{h(\\alpha,h)}}{\\partial{\\tilde{x}_i}}\\) as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\beta_i\\)\n\\(\\tilde{x}_i\\)\n\\(h_{-j}\\)\n\\(h\\)\n\\(h(\\neg \\tilde{x}_i)\\)\n\\(\\delta h\\)\n\\(\\delta \\tilde{x}_i\\)\n\\(h'\\)\n\n\n\n\nT\nT\nT/F/0\n\\(h_{-i}\\)\nF\nF/0/0\nF\nT/0/0\n\n\nT\nF\nT/F/0\nF\nT/F/T\n0/0/0\nF\n0\n\n\nF\nT\nT/F/0\n\\(h_{-i}\\)\n\\(h_{-i}\\)\n0\nT\n0\n\n\nF\nF\nT/F/0\n\\(h_{-i}\\)\n\\(h_{-i}\\)\n0\nT\n0\n\n\n\nTherefore,\n\\[\n\\frac{\\partial{h(\\beta,\\tilde{x})}}{\\partial{\\beta_i}} =\n\\begin{cases}\nF & \\text{ if } \\beta_i = T \\land \\tilde{x}_i = T \\land h_{-i} = T  \\\\\n0 & \\text{ o.w }\n\\end{cases}\n\\] It can be implemented with Heavyside function as follows: \\[\n\\begin{array}{left}\n\\frac{\\partial{y(\\alpha,h)}}{\\partial{h_j}} &=&  -H(\\beta_i)H(\\tilde{x}_i)H(h_{-i})\n\\end{array}\n\\]",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sparse BNN Layer</span>"
    ]
  },
  {
    "objectID": "sparse_bnn_layer.html#sqcup-sor-select-and-or-block",
    "href": "sparse_bnn_layer.html#sqcup-sor-select-and-or-block",
    "title": "Sparse BNN Layer",
    "section": "\\(\\sqcup\\) SOR: Select and OR Block",
    "text": "\\(\\sqcup\\) SOR: Select and OR Block\n\nLogic: Select and OR the inputs\nWe want to realize a module which takes m-inputs, selects some of the m-inputs, and OR’s them. \\[\n\\begin{array}{left}\ny(\\alpha, h) &=& \\sqcup_{j=1}^{m} \\alpha_j \\odot h_j\n\\end{array}\n\\] For convenience, we write them recursively, to ease up the derivation, \\(y = \\sqcup(\\alpha_j \\odot h_j, y_{-j})\\), where \\(y_{-j} = \\sqcup_{k=1, \\neq j  }^{m} \\alpha_k \\odot h_k\\). We can see that \\(y = y_{-j} \\text { if } \\alpha_j = F\\)\n\\[\ny(\\alpha, h) =\n\\begin{cases}\n\\lor(h_{j}, y_{-j}) & \\text{ if }  \\alpha_j = T \\land y_{-j} \\in \\{T,F\\} \\\\\nh_{j} & \\text { if } \\alpha_j = T \\land y_{-j} = 0 \\\\\ny_{-j} & \\text { if } \\alpha_j = F\n\\end{cases}\n\\]\nThis looks much complicated than what it really is trying to say. Rewriting it to get, \\[\ny(\\alpha, h) =\n\\begin{cases}\n0 & \\text{ if } \\alpha_j = F \\,\\, \\forall j \\\\\n\\lor_k h_{k} & \\text{ for }  k \\in \\{ j \\text{ s.t } \\alpha_j = T\\}\n\\end{cases}\n\\]\n\n\nForward Method\nIt can be obtained with Heavyside function as follows: \\[\ny(\\alpha, h) = H\\left(\\sum_{i=1}^m 0.5(\\alpha_j+1) - \\epsilon \\right) \\left( 2 H\\left(\\sum_{j=1}^m 0.25(h_j+1)(\\alpha_i+1) - \\epsilon \\right)-1 \\right)\n\\]\n\n\nBackward Method: Local Gradients\nWe obtain the partial derivatives by observing the Truth Table to get \\(\\frac{\\partial{y(\\alpha,h)}}{\\partial{h_i}}\\) as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha_j\\)\n\\(h_j\\)\n\\(y_{-j}\\)\n\\(y\\)\n\\(y(\\neg h_j)\\)\n\\(\\delta y\\)\n\\(\\delta h_j\\)\n\\(y'\\)\n\n\n\n\nT\nT\nT/F/0\nT\nT/F/F\n0/F/F\nF\n0/T/T\n\n\nT\nF\nT/F/0\nT/F/F\nT\n0/T/T\nT\n0/T/T\n\n\nF\nT\nT/F/0\nT/F/0\nT/F/0\n0\nF\n0\n\n\nF\nF\nT/F/0\nT/F/0\nT/F/0\n0\nF\n0\n\n\n\nTherefore,\n\\[\n\\frac{\\partial{y(\\alpha,h)}}{\\partial{h_i}} =\n\\begin{cases}\nT  & \\text{ if } \\alpha_j = T\\, \\land \\, y_{-j} = F/0 \\\\\n0 & \\text{ o.w } \\\\\n\\end{cases}\n\\]\nIt can be implemented with Heavyside function as follows: \\[\n\\begin{array}{left}\n\\frac{\\partial{y(\\alpha,h)}}{\\partial{h_j}} &=&  H( \\alpha_j)H(-y_{-j} + \\epsilon)\n\\end{array}\n\\] where \\(\\epsilon\\) is a small positive constant.\nWe obtain the partial derivatives by observing the Truth Table to get \\(\\frac{\\partial{y(\\alpha,h)}}{\\partial{\\alpha_i}}\\) as follows:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(\\alpha_j\\)\n\\(h_j\\)\n\\(y_{-j}\\)\n\\(y\\)\n\\(y(\\neg \\alpha_j)\\)\n\\(\\delta y\\)\n\\(\\delta \\alpha_j\\)\n\\(y'\\)\n\n\n\n\nT\nT\nT/F/0\nT\n\\(y_{-j}\\)\n0\nF\n0\n\n\nT\nF\nT/F/0\nT/F/F\n\\(y_{-j}\\)\n0\nF\n0\n\n\nF\nT\nT/F/0\n\\(y_{-j}\\)\nT\n0/T/0\nT\n0/T/0\n\n\nF\nF\nT/F/0\n\\(y_{-j}\\)\nT/F/F\n0/0/0\nT\n0\n\n\n\nTherefore,\n\\[\n\\frac{\\partial{y(\\alpha,h)}}{\\partial{\\alpha_j}} =\n\\begin{cases}\nT & \\text{ if } \\alpha_j = F\\, \\land \\, y_{-j} = F \\, \\land  \\, h_j = T \\\\\n0 & \\text{ o.w } \\\\\n\\end{cases}\n\\]\nIt can be implemented with Heavyside function as follows: \\[\n\\begin{array}{left}\n\\frac{\\partial{y(\\alpha,h)}}{\\partial{h_i}} &=&  H(\\alpha_j)H(-y_{-j})H(h_{j})\n\\end{array}\n\\] where \\(\\epsilon\\) is a small positive constant.\nFinally, we can put it all together",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sparse BNN Layer</span>"
    ]
  },
  {
    "objectID": "sparse_bnn_layer.html#sparse-bnn-layer",
    "href": "sparse_bnn_layer.html#sparse-bnn-layer",
    "title": "Sparse BNN Layer",
    "section": "Sparse BNN Layer",
    "text": "Sparse BNN Layer\nA Sparse BNN Layer is a Boolean Neural Network that maps an N-dimensional input to an K-dimension output, with a hyper parameter \\(H\\) that controls the layer complexity, resulting in a total of \\(H(2N+K)\\) learnable Boolean weights.\n\\[\n\\begin{array}{left}\ny_{k} &=& \\sqcup_{j=1}^{m} \\alpha_{j}^{k} \\odot  h_j \\, & \\text{: SOR}\\\\\nh_j &=& \\sqcap_{i=1}^{n}  \\beta_i^j  \\odot \\tilde{x}_i^j \\, & \\text{: SAND} \\\\\n\\tilde{x}_i^j  &=& w_i^j \\oplus x_i \\, & \\text{: Flip}\n\\end{array}\n\\] where \\(n=1,\\dots,N\\), \\(j=1,\\dots,H\\) \\(k=1,\\dots,K\\).\nCaution\nNote that all weights and inputs will be strictly in $, i.e., $\\(w_i^j, \\beta_i^j, \\alpha_j, x_i \\in {-1,1}\\) and only gradients can be in \\(\\{-1,0,1\\}\\).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Sparse BNN Layer</span>"
    ]
  },
  {
    "objectID": "interfaces.html",
    "href": "interfaces.html",
    "title": "Interfaces",
    "section": "",
    "text": "Pure BNN\nFirst consider the case where BNN is all there is. No upstream or downstream blocks.\nWe need codings that convert analog to digital and digital (binary) to analog but these converters act like pre- and post-processors (i.e., they are not part of the training jobs).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interfaces</span>"
    ]
  },
  {
    "objectID": "interfaces.html#pure-bnn",
    "href": "interfaces.html#pure-bnn",
    "title": "Interfaces",
    "section": "",
    "text": "Bool-in, Bool-out:\nAny boolean truth tables and digital circuits. Example - parity checker!\nReal-in, Bool-out:\nMost binary classification problems fall under this category. One Hello world example is the Iris Flower classification problem. Take the features measured on a Flower and identify the Flower class. Here, we need to convert \\(x \\in \\mathcal{R}^n\\) to \\(\\{-1,1\\}^p\\) for some \\(p\\), and chain BNN layers. Binary Classification labels can be coded as \\(\\{-1,1\\}\\). By extension, multi-label or multi-class can be handled as multi-valued Truth Tables. We need to convert the real inputs to Boolean valued inputs.\nReal-in, Real-out:\nA more complicated problem is when the outputs are real valued. Consider a regression problem with real valued output. We need to code real-valued signal and realize this as a multi-valued Truth table.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interfaces</span>"
    ]
  },
  {
    "objectID": "interfaces.html#mixed-bnn",
    "href": "interfaces.html#mixed-bnn",
    "title": "Interfaces",
    "section": "mixed BNN",
    "text": "mixed BNN\n\nReal-in, Bool-out:\nHere, a BNN Layer is appended to DNN. The DNN layers are trainable which produce real valued outputs. An example could be to train an image classifier based on ResNet head, which also needs to be trained. How do we design the interface (A/D converter) that flows the gradients from BNN to the DNN?\nReal-in, Real-out:\nHere, a BNN layer is sandwiched between two DNN layers (or modules). The BNN receives a real valued input and has to pass a continuous valued signal for the downstream DNN layer. What would the D/A converter look like?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interfaces</span>"
    ]
  },
  {
    "objectID": "interfaces.html#converters-not-trainable",
    "href": "interfaces.html#converters-not-trainable",
    "title": "Interfaces",
    "section": "Converters (not trainable)",
    "text": "Converters (not trainable)\nThe A/D and D/A converters pre-process and post-process the data, and therefore can be considered as not part of the BNN.\n\nADC Coder:\n\nQuantile Binning\nConsider all inputs to be an n-dimensional real valued input. For every dimension, compute the CDF, divide it into bins of equal width, map the input feature to the bin it points to. The bins are one-hot coded. See sklearn’s KBinsDiscretizer\n\n\n\nDAC Coder:\n\nBit Plane Coding\nQuantize the output to the precision needed. Do a bit-plane coding. Note that errors on MSBs are much costlier than the LSBs. Further, we are not interesting in compressing the bit-planes - just use the bit-planes to code a quantized real-valued signal so that BNN can be learnt on them.\nAfter predicting the bit-planes, de-quantize.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interfaces</span>"
    ]
  },
  {
    "objectID": "interfaces.html#adapters-trainable",
    "href": "interfaces.html#adapters-trainable",
    "title": "Interfaces",
    "section": "Adapters (trainable)",
    "text": "Adapters (trainable)\nThe A/D and D/A adapters must allow BNN Layer to be added before/after a DNN layer, and enable training end-to-end.\n\nADC Layer:\n\nRandom Binning Features\nSee paper.\n\n\nCompressive Sensing:\nMap real input \\(x_{n \\times 1}\\) to \\(b^{in}_{m \\times 1} = \\text{sign}(\\Phi x)\\), with \\(\\Phi \\sim N(0,1)\\). It is possible to have \\(\\Phi\\) from \\(\\{0,1\\}\\) as well. See 1-bit Compressive Sensing paper\nForward Pass\ntbd\nBackward Pass\ntbd\n\n\n\nDAC Layer:\n\nCompressive Sensing:\nProblem: Given a signs alone, recover a real-valued sparse signal, given the sensing matrix. That is, Recover \\(y_{k \\times 1} \\in \\mathcal{R}^k\\) from \\(b^{out}_{m \\times 1} \\in \\{-1,1\\}^m\\) given a sensing matrix \\(\\Phi\\) which is hypothesized to have generated the measurements \\(y =  \\Phi b\\).\nSee the papers\n\nRobust 1-Bit Compressive Sensing via Binary Stable Embeddings of Sparse Vectors\nOne-bit Compressed Sensing: Provable Support and Vector Recovery\nAre Straight-Through gradients and Soft-Thresholding all you need for Sparse Training?\nLearning Fast Approximations of Sparse Coding\n\nForward Pass\ntbd\nBackward Pass\ntbd",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Interfaces</span>"
    ]
  }
]