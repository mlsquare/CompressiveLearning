[
  {
    "objectID": "bolt/notebooks/dev.html",
    "href": "bolt/notebooks/dev.html",
    "title": "Boolean Variation",
    "section": "",
    "text": "Boolean Variation\nBOLD: Boolean Logic Deep Learning introduced a mathematical theory to caculuate graidents on Boolean Variables. Below are some necessary info to realize the truth tables, define a Boolean Deep Learning model, and train the network using Backprop. See the paper for details. The following are taken from the paper.",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bolt/notebooks/dev.html#definition",
    "href": "bolt/notebooks/dev.html#definition",
    "title": "Boolean Variation",
    "section": "Definition",
    "text": "Definition\n\nThree-value and Mixed logic\n\nDefine \\(\\mathcal{M} \\equiv \\mathcal{B} \\cup \\{0\\}\\) with logic connectives defined according to those of Boolean logic as follows.\n\nFirst, the negation is: \\(\\neg True = False\\), \\(\\neg False = True\\), and \\(\\neg 0 = 0\\).\nSecond, let \\(\\mathcal{L}\\) be a logic connective, denote by \\(\\mathcal{L}_{\\mathcal{M}}\\) and \\(\\mathcal{L}_{\\mathcal{B}}\\) when it is in \\(\\mathcal{M}\\) and in \\(\\mathcal{B}\\), respectively, then \\(\\mathcal{L}_{\\mathcal{Mb}}(a,b) = \\mathcal{L}_{\\mathcal{Bb}}(a,b)\\) for \\(a, b \\in \\mathcal{B}\\) and \\(\\mathcal{L}_{\\mathcal{M}}(a,b) = 0\\) otherwise.\n\nDenote by \\(\\mathcal{L}\\) a logic set (e.g., \\(\\mathcal{B}\\) or \\(\\mathcal{M}\\)), \\(\\mathcal{R}\\) the real set, \\(\\mathcal{Z}\\) the set of integers, \\(\\mathcal{N}\\) a numeric set (e.g., \\(\\mathcal{R}\\) or \\(\\mathcal{Z}\\)), and \\(\\mathcal{D}\\) a certain set of \\(\\mathcal{L}\\) or \\(\\mathcal{N}\\).\nFor \\(x \\in \\mathcal{N}\\), its logic value denoted by \\(x_{logic}\\) is given as \\(x_{logic} = True \\Leftrightarrow x &gt; 0\\), \\(x_{logic} = False \\Leftrightarrow x &lt; 0\\), and \\(x_{logic} = 0 \\Leftrightarrow x = 0\\).\nThe magnitude of a variable \\(x\\), denoted \\(|x|\\), is defined as its usual absolute value if \\(x \\in \\mathcal{N}\\). And for \\(x \\in \\mathcal{L}\\): \\(|x| = 0\\) if \\(x = 0\\), and \\(|x| = 1\\) otherwise.\nFor \\(\\mathcal{L}\\) a logic connective of \\(\\mathcal{L}\\) and variables \\(a\\), \\(b\\), operation \\(c = \\mathcal{L}(a, b)\\) is defined such that \\(|c| = |a||b|\\) and \\(c_{logic} = \\mathcal{L}(a_{logic}, b_{logic})\\).\n\n\n\nCalculus\n\nA variable x is (extended) Boolean variable with the following encoding \\(x \\in \\{-1,0,1\\}\\) where \\(-1\\) represents logical \\(False\\), \\(+1\\) represents logical \\(True\\), and \\(0\\) represents \\(Ignore\\). We can call this extended Boolean domain.\nOrder relations \\(&lt;\\) and \\(&gt;\\) in \\(\\mathcal{B}\\) are defined as follows: \\(False &lt; True\\), and \\(True &gt; False\\).\nFor \\(a, b \\in \\mathcal{B}\\), the variation from \\(a\\) to \\(b\\), denoted \\(\\delta(a \\to b)\\), is defined as: \\(\\delta(a \\to b) \\equiv True\\) if \\(b &gt; a\\), \\(\\equiv 0\\) if \\(b = a\\), and \\(\\equiv False\\) if \\(b &lt; a\\).\nFor \\(f \\in \\mathcal{F}(\\mathcal{B}, \\mathcal{D})\\), \\(\\forall x \\in \\mathcal{B}\\), write \\(\\delta f(x \\to \\neg x) := \\delta(f(x) \\to f(\\neg x))\\). The variation of \\(f\\) w.r.t \\(x\\), denoted \\(f'(x)\\), is defined as: \\(f'(x) \\equiv \\text{xnor}(\\delta(x \\to \\neg x), \\delta f(x \\to \\neg x))\\).\n\nFor simplicity, we will write \\(\\delta f\\) to denote \\(\\delta f(x \\to \\neg x)\\). Similarly, \\(\\delta x\\) to denote \\(\\delta (x \\to \\neg x)\\)\nFor details see, Section 3.2 of BOLD: Boolean Logic Deep Learning\n\n\nAND Gate\n\nTruth Table\nl \\(AND(x_1,x_2) \\equiv 0\\) if any of the inputs are 0, by definition.\n\n\nDerivative\nRecall: 1. \\(\\delta(a \\to b) \\equiv True\\) if \\(b &gt; a\\), \\(\\equiv 0\\) if \\(b = a\\), and \\(\\equiv False\\) if \\(b &lt; a\\). 2. \\(f'(x) \\equiv \\text{xnor}(\\delta(x \\to \\neg x), \\delta f(x \\to \\neg x))\\).\nThe Truth Table for \\(f(x) = f_a(x) = AND(x,a)\\) is:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\\(a\\)\n\\(x\\)\n\\(\\neg x\\)\n\\(\\delta x\\)\n\\(f(x)\\)\n\\(f(\\neg x)\\)\n\\(\\delta f\\)\n\\(f'\\)\n\n\n\n\nT\nT\nF\nF\nT\nF\nF\nT\n\n\nT\nF\nT\nT\nF\nT\nT\nT\n\n\nF\nT\nF\nF\nF\nF\n0\n0\n\n\nF\nF\nT\nT\nF\nF\n0\n0\n\n\n\nTherefore, \\(f'_{a}(x) = \\text{T} \\text{ iff } a=T, 0 \\text{ o.w}\\)\n\n\n\nOR Gate\n\nTruth Table\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y_{OR}\\)\n\n\n\n\nT\nT\nT\n\n\nT\nF\nT\n\n\nF\nT\nT\n\n\nF\nF\nF\n\n\n\n\\(AND(x_1,x_2) \\equiv 0\\) if any of the inputs are 0, by definition.\n\n\nDerivative\n\\(f'_{a}(x) = \\text{F} \\text{ iff } a=F, 0 \\text{ o.w}\\)\n\n\n\nXOR Gate\n\nTruth Table\n\n\n\n\\(x_1\\)\n\\(x_2\\)\n\\(y_{XOR}\\)\n\n\n\n\nT\nT\nF\n\n\nT\nF\nT\n\n\nF\nT\nT\n\n\nF\nF\nF\n\n\n\n\\(XOR(x_1,x_2) \\equiv 0\\) if any of the inputs are 0, by definition.\n\n\nDerivative\n\\(f'_{a}(x) = \\neg a\\)",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bolt/notebooks/dev.html#and-gate-1",
    "href": "bolt/notebooks/dev.html#and-gate-1",
    "title": "Boolean Variation",
    "section": "AND Gate",
    "text": "AND Gate\n\n# check AND gate. Logic and Derivative\n\ndef test_2ary(gate='AND'):\n    A = [-1,1,0]\n    X = [-1,1,0]\n    data = []\n    for element in itertools.product(*[A,X]):\n        a,x = element\n        xb = Bool(x)\n        ab = Bool(a)\n        if gate == 'AND':\n            z = ab*xb\n        elif gate == 'OR':\n            z = ab+xb\n        else:\n            z = ab^xb\n        \n        z.backward()\n        data.append({\n                'a': a,\n                'x': x,\n                'f': z.data,\n                'f\\'': xb.grad\n            })\n\n    df = pd.DataFrame(data)\n    return df\ndf = test_2ary()\nprint(df)\n\n   a  x  f  f'\n0 -1 -1 -1   0\n1 -1  1 -1   0\n2 -1  0  0   0\n3  1 -1 -1   1\n4  1  1  1   1\n5  1  0  0   0\n6  0 -1  0   0\n7  0  1  0   0\n8  0  0  0   0\n\n\nCan see that \\(f'_a(x)\\) is \\(1\\) only when \\(a=1\\), o.w it is 0.",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bolt/notebooks/dev.html#or-gate-1",
    "href": "bolt/notebooks/dev.html#or-gate-1",
    "title": "Boolean Variation",
    "section": "OR Gate",
    "text": "OR Gate\n\n# check AND gate. Logic and Derivative\ndf = test_2ary(gate='OR')\nprint(df)\n\n   a  x  f  f'\n0 -1 -1 -1  -1\n1 -1  1  1  -1\n2 -1  0  0   0\n3  1 -1  1   0\n4  1  1  1   0\n5  1  0  0   0\n6  0 -1  0   0\n7  0  1  0   0\n8  0  0  0   0\n\n\nCan see that \\(f'_a(x)\\) is \\(-1\\) only when \\(a=-1\\), o.w it is 0.",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bolt/notebooks/dev.html#xor-gate-1",
    "href": "bolt/notebooks/dev.html#xor-gate-1",
    "title": "Boolean Variation",
    "section": "XOR Gate",
    "text": "XOR Gate\n\n# check AND gate. Logic and Derivative\ndf = test_2ary(gate='XOR')\nprint(df)\n\n   a  x  f  f'\n0 -1 -1 -1   1\n1 -1  1  1   1\n2 -1  0  0   0\n3  1 -1  1  -1\n4  1  1 -1  -1\n5  1  0  0   0\n6  0 -1  0   0\n7  0  1  0   0\n8  0  0  0   0\n\n\nCan see that \\(f'_a(x)\\) is \\(\\neg a\\) and is 0 when a is 0.",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "bolt/notebooks/dev.html#compositions",
    "href": "bolt/notebooks/dev.html#compositions",
    "title": "Boolean Variation",
    "section": "Compositions",
    "text": "Compositions\nChain Rule\nFor \\(\\mathcal{B} \\overset{f}{\\to} \\mathcal{B} \\overset{g}{\\to} \\mathcal{D}\\): \\((g \\circ f)'(x) = \\text{xnor}(g'(f(x)), f'(x))\\), \\(\\forall x \\in \\mathcal{B}\\).\nMultivariate Chain Rule\n\nFor \\(x = (x_1, \\ldots, x_n) \\in \\mathcal{B}^n\\), denote \\(x_{\\neg i} := (x_1, \\ldots, x_{i-1}, \\neg x_i, x_{i+1}, \\ldots, x_n)\\) for \\(n \\ge 1\\) and \\(1 \\leq i \\leq n\\).\nFor \\(f \\in \\mathcal{F}(\\mathcal{B}^n, \\mathcal{B})\\), the (partial) variation of \\(f\\) wrt \\(x_i\\), denoted \\(f'_{i}(x)\\) or \\(\\delta f(x)/\\delta x_i\\), is defined as: \\(f'_{i}(x) \\equiv \\delta f(x)/\\delta x_i \\equiv xnor(\\delta(x_i \\to \\neg x_i), \\delta f(x \\to x_{\\neg i}))\\).\nLet \\(f \\in \\mathcal{F}(\\mathcal{B}^n, \\mathcal{B})\\), \\(n \\geq 1\\), and \\(g \\in \\mathcal{F}(\\mathcal{B}, \\mathcal{B})\\). For \\(1 \\le i \\le n\\),\n\n\\[(g \\circ f)'_i(x) = \\text{xnor}(g'(f(x)), f'_i(x)), \\quad \\forall x \\in \\mathcal{B}^n\\]\n\n\ndef test_Kary(expression = \"x1*x2\", K=2):\n    T = [-1, 1, 0]\n   \n    # Create the Cartesian product of T repeated K times\n    cartesian_product = list(itertools.product(T, repeat=K))\n   \n    # Generate variable names based on the index\n    variable_names = [f'x{i+1}' for i in range(K)]\n\n   \n    data = []\n    # Print the result with variable names and evaluate the expression\n    for product in cartesian_product:\n        # Create a dictionary of variable names and their corresponding values\n        variables = {variable_names[i]: Bool(product[i]) for i in range(K)}\n        \n        # Evaluate the expression using the variables\n        result = eval(expression, {}, variables)\n        result.backward()\n\n        tmp = variables\n        tmp['f: '+ expression] = result.data\n        tmp['f\\'(x1)'] = variables['x1'].grad\n\n        data.append(tmp)\n    \n    df = pd.DataFrame(data)\n    return df\n\nexpression = \"x1 * x2\"\nK = 2\ndf = test_Kary(expression=expression, K=K)\nprint(df)\n\n                x1               x2  f: x1 * x2  f'(x1)\n0  data:-1, grad:0  data:-1, grad:0          -1       0\n1  data:-1, grad:1   data:1, grad:0          -1       1\n2  data:-1, grad:0   data:0, grad:0           0       0\n3   data:1, grad:0  data:-1, grad:1          -1       0\n4   data:1, grad:1   data:1, grad:1           1       1\n5   data:1, grad:0   data:0, grad:0           0       0\n6   data:0, grad:0  data:-1, grad:0           0       0\n7   data:0, grad:0   data:1, grad:0           0       0\n8   data:0, grad:0   data:0, grad:0           0       0\n\n\n\nexpression = \"x1^x2\"\nK = 2\ndf = test_Kary(expression=expression, K=K)\nprint(df)\n\n                 x1                x2  f: x1^x2  f'(x1)\n0   data:-1, grad:1   data:-1, grad:1        -1       1\n1  data:-1, grad:-1    data:1, grad:1         1      -1\n2   data:-1, grad:0    data:0, grad:0         0       0\n3    data:1, grad:1  data:-1, grad:-1         1       1\n4   data:1, grad:-1   data:1, grad:-1        -1      -1\n5    data:1, grad:0    data:0, grad:0         0       0\n6    data:0, grad:0   data:-1, grad:0         0       0\n7    data:0, grad:0    data:1, grad:0         0       0\n8    data:0, grad:0    data:0, grad:0         0       0\n\n\nCan see \\(f_a'(x) = \\neg a\\) and zero whenever any element is 0\nPermutation Invariance\nabc = cbc a+b+c = c+a+b\n\nK = 3\nexpression = \"x1 * x2 * x3\"\ndf1= test_Kary(expression=expression, K=K)\nexpression = \"x3 * x2 * x1\"\ndf2= test_Kary(expression=expression, K=K)\n\n\nprint(np.all(df1[df1.columns[-1]]==df1[df1.columns[-1]]))\nprint(np.all(df1[df1.columns[-2]]==df2[df2.columns[-2]]))\n\nexpression = \"x1 + x2 + x3\"\ndf1= test_Kary(expression=expression, K=K)\n\nexpression = \"x3 + x2 + x1\"\ndf2= test_Kary(expression=expression, K=K)\n\nprint(np.all(df1[df1.columns[-1]]==df1[df1.columns[-1]]))\nprint(np.all(df1[df1.columns[-2]]==df2[df2.columns[-2]]))\n\nTrue\nTrue\nTrue\nTrue\n\n\n\n# check composition\na = Bool(-1);b = Bool(1);c=Bool(1)\nz = b^a+c\nz.backward()\nprint('xor(a,b)\\n', z,a,b,c)\n\nxor(a,b)\n data:-1, grad:1 data:-1, grad:0 data:1, grad:-1 data:1, grad:1\n\n\n\n# check composition\nfrom engine import Bool\nfrom utils import draw_dot\n\n\ny = Bool(-1);\nx1 = Bool(1); x2 = Bool(1)\nw1 = Bool(-1); w2 = Bool(-1); \n\nh1 = x1^w1\nh2 = x2^w2\n\nz = h1*h2\nL = z^y\n\nL.backward()\n\ndraw_dot(L)",
    "crumbs": [
      "Notebooks",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Boolean Variation</span>"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Compressive Learning",
    "section": "",
    "text": "Methods and tools for Compressive Learning.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Compressive Learning</span>"
    ]
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Motivation",
    "section": "",
    "text": "Why Compressive Learning?\nObservation:\nWe are taking extremely large networks (LLMs) and then quantizing them to 1-bit precision. Effectively turning them to Boolean Networks and yet notice that performance does not degrade as much. It begets new questions. See 1-bit LLMs paper for recent results. We can put the questions into few broad categories.\n\nAnalysis\n\nThe Lottery Ticket Hypothesis\nLottery Ticket Hypothesis(LTH) conjectured that some sub networks are as good as the full network and remaining ones can be completely pruned-off without incurring lot of degradation in performance.\nIn the 1-bit LLMs paper, authors showed that, all weights can just come from \\(\\{ -1,0,1\\}\\). It is the extreme form of quantization and pruning combined. We see such extreme form of quantization of many modern LLMs. This is done for computational efficiency (also energy efficiency) at inference time.\nLTH authors observed that initialization mattered though, in the sense that, same network retrained from a different initialization did not achieve the same performance the sub network achieved with the the original initialization. I suspect that it is an artefact of the training (learning) algorithm (and not due to the initialization itself).\nWhat role are the prune-able sub networks playing? Is that about redundancy (serving some other important function such as a error control coding) or they are just plain useless - is not clear unless we investigate in that direction. The LTH paper did not study the brittleness after pruning (AFIK).\nAre the pruned networks acting like “error control codes” in the functional space? Not having them is ok from a prediction (accuracy) point of view , but not ok from robustness (sensitivity) point of view.\nHow we do study generalization ability of such binary networks? and from a build point of view, what is needed to generalize. I hypothesize that smoothness of the functions is central to generalization. So, how we do define and later enforce such smoothness of Boolean Networks.\nA related question - what is the equivalent notion of robustness for Boolean maps?\nHow do we analyze (and develop the theory to analyze) such Boolean functions which map n-dimensional inputs bit streams to m-dimensional output bit streams? Will the theories of error control codes based on groups and fields help?\n\n\n\nBuild & Develop\n\nDo we need to go through the route of Train large models with full precision and quantize or can we Learn in the 1-bit space itself. Drawing inspiration from compressive sensing, can we do Compressive Learning?.\nHow do we train such 1-bit models to begin with? If autograd and backprop made possible the training (with non-trivial enablers like data, compute and massive engineering data center scale feats) - are there efficient learning algos to train 1-bit LLMs?\nBoolean Logic Deep Learning (BOLD) introduced some Variational Theory to define gradients on Boolean variables, and a corresponding chain rule to go with.\nThe implication is that - we can compute the gradients on mixed data types effectively for any computational DAG (all neural networks are such graphs) via backprop algo. But will it work at scale?\n\n\n\nKernel Machines\nCan Kernels bridge the digital (Boolean Networks) and the analog (current full precision LLMs) divide?\nBefore Deep Learning took-off circa 2012, most successful models were Kernel Machines. It was a well researched area, and probably well understood as well, relative to Deep Learning. Some evidence that they can.\n\nTrees are Kernels Machines: Some might argue - what about Trees and Ensembles? They are not Kernel Machines. Under the hood, Random Forests are also Kernel Machines. In an obscure paper Some infinity theory for predictor ensembles, Leo Brieman shows that Random Forests implicitly learn a Laplacian Kernel, asymptotically.\nTransformers are Kernel Machines: In another insightful work, Prof. Rich B who did pioneering work in signal processing in general, and compressive sensing in particular (the 1-pixel camera project, for example), re-derived Transformers like Prof. Yi Ma’s group, but using a different route - by looking at Transformers as Kernel Machines. In A Primal-Dual Framework for Transformers and Neural Networks, an explicit construction of a polynomial kernel was given, whose dual form gives raise to the self-attention block of Transformers. Perhaps this can explain why Transformers were not very competitive to Trees (due to the different inductive biases encoded by their respective Kernels. Not all Kernels are equal).\nSGD is a Kernel Machine Learner: In this paper, Prof. Pedro Domingos shows that every model learned by Gradient Descent is approximately a Kernel Machine. The Kernel happens to be Neural Tangent Kernel (NTK), which seems to be main analytical device to study the behavior of Deep Neural Networks (See this for example).\nRandom Features are Large Scale Kernel Machines: In a time-tested paper, Ali and Ben, show that linear models on random features are Kernel Machines, and they are extremely competitive and simple (to implement and model), at the same time.\nSeparability in High Dimensional Feature space: The conventional wisdom in ML is, transform low dim input to very high (non linear) features space, where modeling becomes easier. Kernels provide that kind of Statistical Machinery to both compute and theorize the development of new models (architectures) and learning algorithms.\n\nBut what about Boolean functions, i.e, functions that map n-dimensional Boolean inputs to m-dimensional Boolean outputs? In this beautiful paper Learning of Boolean Functions Using Support Vector Machines, publised in 2001, Ken Sadohara develops DNF Kernels to be applied along side SVMs. The connection between DNF (Disjunctive Normal Form) and Boolean circuits are rather straightforward to see. Any truth table (coding a Boolean function) can be represented either in Sum-of-Products (SOP) or Product-of-Sum (POS) form, and DNF corresponds to the SOP form. So, if we can construct a model and learn algorithms to go with, such model becomes a universal learner (of any Boolean function). This is the constructionist approach to model(architecture) design.\nSo the thesis is - All modern, empirically proven architectures are a Composition of Learnable Kernel Machines and perhaps we can extend this to Deep Binary Neural Networks as well.\n\n\nAxiomatic Notions of Intelligence\n\nPrinciple of Parsimony and Self-Consistency: In a series of papers, based on decades of work, Prof. Yi Ma and colleagues argued that for intelligence to emerge, certain aspects must be considered such as representations must be parsimonious (not exactly sparse) and they must be self-consistent (to recover the original signal).\nCompression: A key point Prof. Yi Ma emphasizes is that Compression is at the heart of intelligence. Using rate-reduction theory (widely used in communication theory), his group was able to re-derive Transformers, ConvNets, the empirically proven architectures from first principles.\nAbility to learn based on few examples: Intelligent systems should be able to learn based on the feedback loop (error control) with few examples.\nRecollection is not a sign of intelligence: By that account, taking a cue from Prof. Subbarao Khambampati, LLMs are approximate retrieval machines, which do not have any reasoning ability, unless specifically equipped to do so, as the recent works seem to suggest.\n\n\n\nAxiomatic Notions of Perception and Processing\n\nPerceive in low dimensions but with high precision (fidelity).\nProject them on to very high dimensional, parsimonious, self-consistent representations but in low precision (1 bit for eg)\nProcess them in the bit space (energy efficient)\n\nWhat it means is, the interface between the external world and the processor (reasoner or model) is like an A/D Convertor (Analog to Digital converter) and the processor (model) only performs bitwise operations and we can convert the Binary signals back to Analog (D/A converter) for external communication.\nPutting it all together,\n\n\nCompressive Learning\nis to study and develop\n\nComputational Training Stack\n\ndefine gradients on Boolean variables, and a chain rule to go with\ndevelop backprop to scale to large computational DAGs\nPerhaps, use Genetic Algorithms to augment the training\n\nKernels\n\nLearnable DNF Kernels to learn Boolean features or other Universal Learners of Boolean N/Ws in SOP or POS form\nCompose the Kernels depth-wise to retain the expressivity of modern Deep Learning models.\n\nIssues\n\nCan they be trained? Or do they just rote learn?\nCan they generalize?\nCan they be distilled?\nCan they be analyzed?\n\n\nIf successful, an optimistic outlook for these Deep Binary Neural Networks is, they are:\n\nInterpretable: since we can recover the DNF forms\nEnergy-efficient: require bit-wise operations, not needing giant matrix multiplications (so no GPUs)\nASIC-friendly: we may be able compile PyTorch models directly into HDL languages and burn the models on silicon. Like 3D printing, forge your model on silicon.\nSLA-friendly: with high token throughput and low latency\nEdge-friendly: with low memory footprint, can be deployed on edge devices\nEnd-to-End trainable: no need for train-large-then-quantize paradigm - train in the compressed domain itself.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Motivation</span>"
    ]
  }
]