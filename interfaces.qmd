# Interfaces

For a strictly Boolean-in and Boolean-out type functions, the [BNN layer](./bnn_layer.qmd) or the [Sparse BNN Layer](./sparse_bnn_layer.qmd) are supposed to work. For example, learning a parity checker which takes n Boolean inputs and outputs a single Boolean value. But that is somewhat restrictive in a way, if we want wider adoption BNNs. The BNN layer as a replacement for an MLP needs different _adapters_ depending on the type of modeling it is involved in. 

Obviously, BNN only works on Bool variables (which we can all pure p-BNN). But it also matters whether the upstream and downstream blocks are to be co-trained or not (which we can call mixed or m-BNN). It changes the game. We need an ability convert reals to Bools and vice-versa.


Terms used to refer to the interfaces:

- Analog to Digital = A/D = Analog to Digital Converter = ADC
- Digital to Analog = D/A = Digital to Analog Converter = DAC


## Pure BNN

First consider the case where BNN is all there is. No upstream or downstream blocks.

- Bool-in, Bool-out: 

    Any boolean truth tables and digital circuits. Example - parity checker!

- Real-in, Bool-out: 

     Most binary classification problems fall under this category. One _Hello world_ example is the Iris Flower classification problem. Take the features measured on a Flower and identify the Flower class. Here, we need to convert $x \in \mathcal{R}^n$ to $\{-1,1\}^p$ for some $p$, and chain BNN layers. Binary Classification labels can be coded as $\{-1,1\}$. By extension, multi-label or multi-class can be handled as multi-valued Truth Tables. _We need to convert the real inputs to Boolean valued inputs._

- Real-in, Real-out: 

    A more complicated problem is when the outputs are real valued. Consider a regression problem with real valued output. We need to code real-valued signal and realize this as a multi-valued Truth table.

We need codings that convert analog to digital and digital (binary) to analog but these converters act like pre- and post-processors (i.e., they are not part of the training jobs).



## Mixed BNN

- Real-in, Bool-out: 

    Here, a BNN Layer is appended to DNN. The DNN layers are trainable which produce real valued outputs. An example could be to train an image classifier based on ResNet head, which also needs to be trained. How do we design the interface (A/D converter) that flows the gradients from BNN to the DNN?

- Real-in, Real-out: 

    Here, a BNN layer is sandwiched between two DNN layers (or modules). The BNN receives a real valued input and has to pass a continuous valued signal for the downstream DNN layer. What would the D/A converter look like?

 

## Converters (not trainable)

The A/D and D/A converters pre-process and post-process the data, and therefore can be considered as not part of the BNN. 

### ADC Coder:

#### Quantile Binning

Consider all inputs to be an n-dimensional real valued input. For every dimension, compute the CDF, divide it into bins of equal width, map the input feature to the bin it points to. The bins are one-hot coded. See sklearn's [KBinsDiscretizer](https://scikit-learn.org/1.5/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html)

### DAC Coder:

#### Bit Plane Coding

Quantize the output to the precision needed. Do a bit-plane coding. Note that errors on MSBs are much costlier than the LSBs. Further, we are not interesting in compressing the bit-planes - just use the bit-planes to code a quantized real-valued signal so that BNN can be learnt on them.

After predicting the bit-planes, de-quantize.


## Adapters (trainable)

The A/D and D/A adapters must allow BNN Layer to be added before/after a DNN layer, and enable training end-to-end.

### ADC Layer:

#### Random Binning Features

See  [paper](https://people.eecs.berkeley.edu/~brecht/papers/07.rah.rec.nips.pdf). 

####  Compressive Sensing: 

Map real input  $x_{n \times 1}$ to $b^{in}_{m \times 1} = \text{sign}(\Phi x)$, with $\Phi \sim N(0,1)$. It is possible to have $\Phi$ from $\{0,1\}$ as well.

A related idea is to consider $b = \text{sign}(\tau u^Tx ) \text{ s.t } ||u|| = 1, \tau \in (0,1)$.  Here, we can interpret $u$ as the directional vector, and $\tau$ is the scaling factor that measures the Half-space depth. When combined, they can be used to estimate depth quantiles, a generalized notion of quantiles, extended to multivariarte case.  Depth Quantiles, Directional Quantiles, Tukey's Half-spaces are related to Half-spaces fundamental in ML (in SVMs, we refer to them as the separating Hyperplanes, and max-margin algo finds them). 

See

- 1-bit Compressive Sensing [paper](https://arxiv.org/abs/1104.3160)
- [Quantile Tomography: USsing Quantiles with multivariate data](https://arxiv.org/abs/0805.0056)
- [Multivariate quantiles and multiple-output regression quantiles: From L1 optimization to half-space depth](https://arxiv.org/abs/1002.4486).


**Forward Pass**

Input: $x_{n \times 1} \in \mathcal{R}^n$, a real-valued n-dim vector.
Output: $b_{m \times 1} \in \{-1,1\}^m$, a discrete  valued m-dim vector. Typically $m >> n$.

Let $\Phi_{m \times n}$ be a known (non-trainable) matrix. The forward pass is: $b = \text{sign}(\Phi x)$

Choice of $\Phi$

1. $\Phi \sim N(0,1)$ - every element is drawn from standard normal distribution.
2. $\Phi$ - is designed according to 1-bit CS theory suggested [here]([One-bit Compressed Sensing: Provable Support 
and Vector Recovery](https://proceedings.mlr.press/v28/gopi13.pdf))
3. $\Phi$ s.t $\Phi^T \Phi = \text{Diag}(\tau)$ and elements of $\tau$ can be sampled from $U(0,1)$ or spaced at uniform intervals. 

**Backward Pass**

For the forward pass of the form $b = \text{sign}(\Phi x)$

Option-1: With Straight Through Estimator (STE), replacing the non-differential function with Identity, the local derivative is:
$$
\frac{\partial{b}}{\partial{x}} = 
\begin{cases}
\Phi & \text{ if } |x| < 1 \\
0 & \text{ o.w }
\end{cases}
$$

Option-2: We implement a smooth approximation of the $\text{sign}$ function, with a scheduler that controls the approximation (smoothness) over the course of the training. Consider, 
$\text{sign}(x) = \lim_{\alpha \to \infty} \text{tanh}(\alpha x)$

$$
\frac{\partial{b}}{\partial{x}} = 
\begin{cases}
\alpha\text{sech}^2(\alpha\Phi) & \text{ if } |x| < 1 \\
0 & \text{ o.w }
\end{cases}
$$

Obviously, $\alpha$ can not be too large. During the course of the training, it can follow a scheduling regime. It being constant is one of the choices for example. If $\alpha$ is fixed, and we use $\text{tanh}$ function in `torch`, we do not need to code any custom `backprop` functions.




### DAC Layer:

####  Compressive Sensing:

Problem: 
Given a signs alone, recover a real-valued sparse signal, given the sensing matrix. That is,
Recover $y_{k \times 1} \in \mathcal{R}^k$ from $b^{out}_{m \times 1} \in \{-1,1\}^m$ given a sensing matrix $\Phi$ which is hypothesized to have generated the measurements $b =  \Phi y$.

See the papers

1. [Robust 1-Bit Compressive Sensing via Binary Stable Embeddings of Sparse Vectors](https://arxiv.org/abs/1104.3160)
2. [One-bit Compressed Sensing: Provable Support and Vector Recovery](https://proceedings.mlr.press/v28/gopi13.pdf)
3. [Are Straight-Through gradients and Soft-Thresholding all you need for Sparse Training?](https://arxiv.org/abs/2212.01076)
4. [Learning Fast Approximations of Sparse Coding](https://icml.cc/Conferences/2010/papers/449.pdf)
5. [Revisiting FISTA for Lasso: Acceleration Strategies Over The Regularization Path](https://www.esann.org/sites/default/files/proceedings/legacy/es2018-81.pdf)

**Forward Pass**

At the heart, recovering sparse signal $y$ from an observed binary signal $b$ is exactly the linear regression with $l_1$ penalty, and it can be solved by iterative optimization techniques like projected coordinate descent, ISTA, FISTA, among others. We can interpret each time step of the  the optimization process as a layer in the Deep Learning. The number of steps in the optimization correspond to the depth of the unrolling. 

We want to write the optimization step for solving $b = \Phi y$, subject some constraints on the sparsity of the recovered signal. We consider the FISTA steps. See [this](https://www-users.cse.umn.edu/~boley/publications/papers/fistaPaperP.pdf) for reference. We are seeking a solution to 

$$
\min_{y \in \mathcal{R}^k } = \frac{1}{2} || \Phi y - b || + \lambda ||y||_{1}
$$
which is precisely the lasso linear regression. The projected gradient descent provides an estimate to the solution, outlined below. 


1. Initialize: $y_{0}, y_{-1}=0, \eta_0,=1$. Input $L, \lambda$. For $t=1,2,..,T$ Run T steps.
2. $\eta_{t} = \frac{1}{2}\left(1+ \sqrt{1+4 \eta_{t-1}^2} \right)$
3. $w_{t} = y_{t-1} + \frac{\eta_{t-1}-1}{\eta_{t}}(y_{t-1}-y_{t-2})$ 
4. $y_{t} = S_{\lambda/L}( w_t - \frac{1}{L} \left( [\Phi^T \Phi] w_t + \frac{1}{L}\Phi^T b \right) )$
5. Assign $y = y_T$ as the output to be connected to downstream layer.

Here $S_{\gamma}$ is the soft-thresholding operator defined as $S_{\gamma}(x) = \text{sign}(x) \text{ReLU}(|x|-\gamma)$ and $L$ is an estimate of the Lipschitz constant.



**Backward Pass**

In the Forward Pass, except for the $S_{\gamma}$ -- all are differentiable operators. Below are some options.

Option-1: We can define a smooth version of $S_{\gamma}$ as follows:
$$
S_{\gamma} = 
\begin{cases}
x-\gamma(1-\epsilon) & \text{ if } x \ge \gamma \\
\epsilon x & \text{ if  }  -\gamma < x < \gamma \\
x-\gamma(1-\epsilon) & \text{ if } x \le \gamma \\
\end{cases}
$$
We can see it exactly fits when $\epsilon=0$. Its gradients can now be defined:
$$
\frac{\partial S_{\gamma}}{\partial x} = 
\begin{cases}
1 & \text{ if } x \ge \gamma \\
\epsilon  & \text{ if  }  -\gamma < x < \gamma \\
1 & \text{ if } x \le \gamma \\
\end{cases}
$$


Option-2: Like before, replace $\text{sign}$ function with its smooth version. For example,  $S_{\gamma}(x) = \text{tanh}(x) \text{ReLU}(|x|-\gamma)$. (check if $|x$| returns `grad` in `Torch`).

Option-3: Replace the soft-thresholding with identify, and pass the gradients.

Note: If the sensing matrix $\Phi$ is carefully chosen (Unitary, for example), the FISTA becomes lot simpler, and some terms can be cached, the key recurrence expression simplifies to
$$
\begin{array}{left}
y_{t} &=& S_{\lambda/L}( w_t - \frac{1}{L} \left( [\Phi^T \Phi] w_t + \frac{1}{L}\Phi^T b \right) )
& \approx & S_{\lambda/L}(\tilde{w}_t)
\end{array}
$$
where $\tilde{w}_t = \tilde{a} w_t + \tilde{b}$, with $\tilde{a} = (1-1/L), \tilde{b}= \frac{1}{L}\Phi^T b$ that are constant through the steps.





